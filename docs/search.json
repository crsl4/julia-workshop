[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julia Workshop for Data Science",
    "section": "",
    "text": "Schedule\n\n\n\nTime\nTopic\nPresenter\n\n\n\n\n11:00 - 11:30\nSession 1: Get Started with Julia\nClaudia Solis-Lemus\n\n\n11:30 - 12:30\nSession 2a: Data Tables and Arrow files\nDouglas Bates\n\n\n12:30 - 1:00\nSession 2b: Interval Overlap\nDouglas Bates\n\n\n1:00 - 2:00\nLunch break\n\n\n\n2:00 - 3:00\nSession 3: Model fitting\n\n\n\n3:00 - 4:00\nSession 4: Hands-on exercise\nSam Ozminkowski and Bella Wu\n\n\n4:00 - 4:15\nCoffee break\n\n\n\n4:15 - 5:00\nPresentation of selected participants’ scripts and Q&A\n\n\n\n5:00 - 5:30\nSession 5: Other important Data Science tools\nClaudia Solis-Lemus\n\n\n5:30 - 6:00\nSession 6: Conclusions and questions\nClaudia Solis-Lemus\n\n\n\n\n\nIn preparation for the workshop\nParticipants are required to follow the next steps before the day of the workshop:\n\nGit clone the workshop repository: git clone https://github.com/crsl4/julia-workshop.git\nInstall Julia. The recommended option is to use JuliaUp:\n\n\nWindows: winget install julia -s msstore\nMac and Linux: curl -fsSL https://install.julialang.org | sh\nHomebrew users: brew install juliaup\n\nAfter JuliaUp is installed, you can install different Julia versions with:\njuliaup add release  ## installs release version\njuliaup add rc       ## installs release candidate version\njuliaup st           ## status of julia versions installed\njuliaup default rc   ## make release candidate version the default\n\nChoose a dataset along with a script to analyze it written in another language (R or python) as we will spend part of the workshop translating participants’ scripts to Julia."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This tutorial will provide an introduction to key Data Science tools in Julia such as data management with Arrow.jl and Tables.jl and (Generalized) linear mixed models with GLMM.jl and MixedModels.jl. Unlike widely used R packages, all packages that we will describe are written 100% in Julia thus illustrating the language’s potential to overcome the two-language problem.\nThis tutorial will appeal to anyone interested in learning more about Julia and some of the existing Julia packages that are already available for Statistics and Data Science. In addition to lectures, participants will engage in hands-on exercises. For example, participants will bring a dataset of their choice along with an existing script written in another language (R or python) that performs certain data analyses. During the tutorial, participants will translate their work to Julia in-order to compare running times and ease of programming."
  },
  {
    "objectID": "session1-get-started.html",
    "href": "session1-get-started.html",
    "title": "1: Getting started with Julia",
    "section": "",
    "text": "“We want a language that is\n\nopen source\nwith the speed of C\nobvious, familiar mathematical notation like Matlab\nas usable for general programming as Python\nas easy for statistics as R\nas natural for string processing as Perl\nas powerful for linear algebra as Matlab\nas good at gluing programs together as the shell\ndirt simple to learn, yet keeps the most serious hackers happy”\n\n\n\n\n\nComparison with other languages: Julia touts its speed edge over Python and R\nUsed for large-scale projects like CliMA 0.1: a first milestone in the next generation of climate models\n\nClimateMachine.jl\n\nJulia adoption accelerated at a rapid pace in 2020:\n\n\n\n\n\n\n\n\nUse the REPL as a sophisticated calculator\nRealize that you are repeating many operations, so you decide to write some functions\nTo organize all your functions, you begin scripting\nYou want to share your code with others and thus, you want to write a package\nYour package is actually used by others and thus, it should be optimized and have good performance\n\nJulia offers many advantages to data science programmers such as avoid the two-language problem and existing tools that allows programmers to write efficient code without having to write everything from scratch!"
  },
  {
    "objectID": "session1-get-started.html#how-do-i-write-code",
    "href": "session1-get-started.html#how-do-i-write-code",
    "title": "1: Getting started with Julia",
    "section": "How do I write code?",
    "text": "How do I write code?"
  },
  {
    "objectID": "session1-get-started.html#installing-dependencies-in-a-project-environment",
    "href": "session1-get-started.html#installing-dependencies-in-a-project-environment",
    "title": "1: Getting started with Julia",
    "section": "Installing dependencies in a project environment",
    "text": "Installing dependencies in a project environment\nThere are two alternatives:\n\nUsing an existing project with dependencies already in Project.toml (this will be the case when you are collaborating with someone that already set up the project dependencies)\nSet up the dependencies for your project on your own (this will be the case if your project is new)\n\nFor this workshop, participants will use the workshop GitHub repository as the existing project that already has all the dependencies. However, we also show the steps below to create a new project from scratch.\n\n1. Working on an existing project environment\nGit clone the repository:\ngit clone https://github.com/crsl4/julia-workshop.git\nOpen julia and activate the package with:\njulia --project\nAlternatively, you can open julia normally, and type ] activate .\nThen, instantiate the package (install dependencies) in the package mode ]:\n(julia-workshop) pkg> instantiate\n(julia-workshop) pkg> update\nNow, you should be able to follow along the workshop commands. Trouble-shooting might be needed when we reach the interoperability with Python and R as certain libraries or packages might need to be installed too.\n\n\n2. Creating a new project environment\nCreate a folder in the terminal:\nmkdir myProject\ncd myProject\nOpen Julia inside your folder, and activate your environment with:\n(@v1.8) pkg> activate .\nInstall the packages that we need. For example, the packages needed for today’s workshop are:\njulia> ENV[\"PYTHON\"] = \"\"\n(myproject) pkg> add PyCall\n(myproject) pkg> add IJulia\n(myproject) pkg> build IJulia\n(myproject) pkg> add MixedModels\n(myproject) pkg> add RCall\n(myproject) pkg> add Arrow\n(myproject) pkg> add DataFrames\n(myproject) pkg> add Tables\n(myproject) pkg> add RangeTrees\nNote: This will create a whole new conda environment, so it will take up space in memory.\nTwo files are noteworthy:\n\nProject.toml: Defines project\nManifest.toml: Contains exact list of project dependencies\n\nshell> head Project.toml\n[deps]\nArrow = \"69666777-d1a9-59fb-9406-91d4454c9d45\"\nDataFrames = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\"\nIJulia = \"7073ff75-c697-5162-941a-fcdaad2a7d2a\"\nMixedModels = \"ff71e718-51f3-5ec2-a782-8ffcbfa3c316\"\nPyCall = \"438e738f-606a-5dbb-bf0a-cddfbfd45ab0\"\nRCall = \"6f49c342-dc21-5d91-9882-a32aef131414\"\nTables = \"bd369af6-aec1-5ad0-b16a-f7cc5008161c\"\n\n\nshell> head Manifest.toml\n# This file is machine-generated - editing it directly is not advised\n\njulia_version = \"1.8.0-beta3\"\nmanifest_format = \"2.0\"\nproject_hash = \"01baf737705b090869a607b779c699f83bbeb154\"\n\n[[deps.ArgTools]]\nuuid = \"0dad84c5-d112-42e6-8d28-ef12dabb789f\"\nversion = \"1.1.1\"\nLook at your Project.toml and Manifest.toml files after installation. They have all the necessary information about your session.\nThe packages have a uuid string which is the universally unique identifier.\nMore on the Project.toml and Manifest.toml files here."
  },
  {
    "objectID": "session1-get-started.html#easily-share-with-collaborators",
    "href": "session1-get-started.html#easily-share-with-collaborators",
    "title": "1: Getting started with Julia",
    "section": "Easily share with collaborators",
    "text": "Easily share with collaborators\nShare your project to colleagues. Send your entire project folder to your colleague, and all they need to do is:\njulia> cd(\"path/to/project\")\npkg> activate .\npkg> instantiate\nAll required packages and dependencies will be installed. Scripts that run in your computer will also run in their computer."
  },
  {
    "objectID": "session2a-tables-and-arrow.html",
    "href": "session2a-tables-and-arrow.html",
    "title": "2a: Data Tables and Arrow files",
    "section": "",
    "text": "Code\nusing Arrow             # Arrow storage and file format\nusing CategoricalArrays # similar to the factor type in R\nusing CSV               # read/write CSV and similar formats\nusing Downloads         # file downloads\nusing DataFrames        # versatile tabular data format\nusing GZip              # utilities for compressed files\nusing RCall             # run R within Julia\nusing Tar               # tar archive utilities\n\n\nDownloads, Gzip and Tar are included just to demonstrate downloading and extracting tar files within Julia.\nYou may find it easier to simply click on the download link and extract the files by hand.\nDownloading and extracting within Julia, as shown here, has a greater chance of working across various operating systems and environments in a workshop like this.\nRCall is included to show the use of other systems running within Julia.\nYou can instead use your favorite environment, such as jupyterlab or RStudio, to run Python or R\nNote that the quarto notebooks for these notes are easily converted, e.g. quarto convert notebookname.qmd, to Jupyter notebooks.\n\n\n\n\n\n\n\nNotes on Julia syntax\n\n\n\n\n\nBoxes like this contain comments on Julia syntax and semantics in code examples. The character in the upper right corner of the box is a toggle to expand or collapse the contents of the box."
  },
  {
    "objectID": "session2a-tables-and-arrow.html#task-and-sample-data",
    "href": "session2a-tables-and-arrow.html#task-and-sample-data",
    "title": "2a: Data Tables and Arrow files",
    "section": "Task and sample data",
    "text": "Task and sample data\n\nLi Heng provides benchmark code and sample data for comparing programming languages on Bioinformatics tasks in his biofast repository.\nOne of these tasks, an interval query, takes two .bed files to compare.\nOne file, ex-anno.bed, contains a reference set of intervals; the other, ex-rna.bed, contains target intervals.\nFor each target interval, determine which reference intervals overlap with it.\nIn the benchmark both the number of reference intervals that overlap with a target and the proportion of the target covered by the overlap are computed.\nNote that the calculation of the proportion of overlap must allow for overlapping intervals in the reference set, as shown in this figure"
  },
  {
    "objectID": "session2a-tables-and-arrow.html#extract-the-files-of-interest-if-not-already-present",
    "href": "session2a-tables-and-arrow.html#extract-the-files-of-interest-if-not-already-present",
    "title": "2a: Data Tables and Arrow files",
    "section": "Extract the files of interest (if not already present)",
    "text": "Extract the files of interest (if not already present)\n\nisdir(datadir) || mkdir(datadir)\nbedfnms = joinpath.(datadir, [\"ex-anno.bed\", \"ex-rna.bed\"])\ntoextract = filter(!isfile, bedfnms)  # don't overwrite existing files\nif !isempty(toextract)\n  tmpdir = gzopen(tarball, \"r\") do io\n    Tar.extract(h -> in(h.path, toextract), io)\n  end\n  for pathnm in toextract\n    mv(joinpath(tmpdir, pathnm), pathnm; force = true)\n  end\nend\nfilter(endswith(\".bed\"), readdir(datadir))\n\n2-element Vector{String}:\n \"ex-anno.bed\"\n \"ex-rna.bed\"\n\n\n\n\n\n\n\n\nDot vectorization\n\n\n\n\n\nThe call joinpath.(datadir, [\"ex-anno.bed\", \"ex-rna.bed\"]) is an example of dot vectorization.\n\n\n\n\n\n\n\n\n\nStabby lambda syntax for anonymous functions\n\n\n\n\n\nThe expression h -> in(h.path, toextract) defines an anonymous function, in the “stabby lambda” syntax, to be used as a predicate in Tar.extract.\n\nmethods(Tar.extract)\n\n# 4 methods for generic function extract: extract(predicate::Function, tarball::Union{Base.AbstractCmd, AbstractString, IO}) in Tar at /home/bates/.julia/juliaup/julia-1.8.0-rc1+0~x64/share/julia/stdlib/v1.8/Tar/src/Tar.jl:218  extract(predicate::Function, tarball::Union{Base.AbstractCmd, AbstractString, IO}, dir::Union{Nothing, AbstractString}; skeleton, copy_symlinks, set_permissions) in Tar at /home/bates/.julia/juliaup/julia-1.8.0-rc1+0~x64/share/julia/stdlib/v1.8/Tar/src/Tar.jl:218  extract(tarball::Union{Base.AbstractCmd, AbstractString, IO}) in Tar at /home/bates/.julia/juliaup/julia-1.8.0-rc1+0~x64/share/julia/stdlib/v1.8/Tar/src/Tar.jl:248  extract(tarball::Union{Base.AbstractCmd, AbstractString, IO}, dir::Union{Nothing, AbstractString}; skeleton, copy_symlinks, set_permissions) in Tar at /home/bates/.julia/juliaup/julia-1.8.0-rc1+0~x64/share/julia/stdlib/v1.8/Tar/src/Tar.jl:248 \n\n\n\n\n\n\n\n\n\n\n\ndo/end blocks\n\n\n\n\n\nThe ‘do/end’ block is yet another way of writing an anonymous function passed as the first argument in the call to gzopen, even though it occurs after that call in the code.\n\nmethods(gzopen)\n\n# 4 methods for generic function gzopen: gzopen(fname::AbstractString) in GZip at /home/bates/.julia/packages/GZip/JNmGn/src/GZip.jl:264  gzopen(fname::AbstractString, gzmode::AbstractString) in GZip at /home/bates/.julia/packages/GZip/JNmGn/src/GZip.jl:263  gzopen(fname::AbstractString, gzmode::AbstractString, gz_buf_size::Integer) in GZip at /home/bates/.julia/packages/GZip/JNmGn/src/GZip.jl:236  gzopen(f::Function, args...) in GZip at /home/bates/.julia/packages/GZip/JNmGn/src/GZip.jl:267 \n\n\nThe effect is to uncompress the file into a stream, process the stream in this anonymous function, then close the stream.\n\n\n\n\n\n\n\n\n\nExtract to temporary directory\n\n\n\n\n\nBecause Tar.extract is conservative about overwriting files and requires that the directory into which the files are extracted be empty, we extract to a freshly-created temporary directory then move the files to the desired location.\n\n\n\n\n\n\n\n\n\nFully qualified names for many packages of utilities\n\n\n\n\n\n\nIt is common for packages providing utilities to avoid name conflicts by not exporting any names from their namespace (or Module).\nThe fully qualified name, Tar.extract, can always be used - similar to Python naming conventions.\nIf a package exports a name, say foo, then after the using FooPackage directive, the unqualified name foo can be used.\nThe varinfo function provides a listing of the names exported by a Package (formally the package’s Module).\nCompare the result below with that of, say, varinfo(DataFrames).\n\nvarinfo(Tar)\n\n\n\nname\nsize\nsummary\n\n\n\n\nTar\n859.204 KiB\nModule"
  },
  {
    "objectID": "session2a-tables-and-arrow.html#reading-arrow-files-in-julia",
    "href": "session2a-tables-and-arrow.html#reading-arrow-files-in-julia",
    "title": "2a: Data Tables and Arrow files",
    "section": "Reading Arrow files in Julia",
    "text": "Reading Arrow files in Julia\n\nannotbl = Arrow.Table(joinpath(datadir, \"ex-anno.arrow\"))\n\nArrow.Table with 573806 rows, 3 columns, and schema:\n :chromo  String\n :start   Int32\n :stop    Int32\n\nwith metadata given by a Base.ImmutableDict{String, String} with 1 entry:\n  \"url\" => \"https://github.com/lh3/biofast/releases/tag/biofast-data-v1\"\n\n\n\nAlthough the schema describes the chromo column as Strings the values are dictionary encoded such that each value is represented by one byte.\n\n\ntypeof(annotbl.chromo)\n\nArrow.DictEncoded{String, Int8, Arrow.List{String, Int32, Vector{UInt8}}}\n\n\n\n@time rnatbl = Arrow.Table(rnaarrownm)\n\n  0.125986 seconds (326 allocations: 77.842 MiB, 33.96% gc time)\n\n\nArrow.Table with 4685080 rows, 3 columns, and schema:\n :chromo  String\n :start   Int32\n :stop    Int32\n\nwith metadata given by a Base.ImmutableDict{String, String} with 1 entry:\n  \"url\" => \"https://github.com/lh3/biofast/releases/tag/biofast-data-v1\"\n\n\n\nWe can use operations like split-apply-combine on these tables to summarize properties\n\n\nannogdf = groupby(DataFrame(annotbl), :chromo)\nrnagdf = groupby(DataFrame(rnatbl), :chromo)\ninnerjoin(\n  combine(rnagdf, nrow => :nrna),\n  combine(annogdf, nrow => :nanno);\n  on = :chromo,\n)\n\n\n24 rows × 3 columnschromonrnanannoStringInt64Int641chr01453114527892chr02298278425633chr03251858347694chr04132781225005chr05225898260856chr06307747249357chr07217038276138chr08141205216039chr091651812018910chr101622562006111chr112865843402112chr122920023352413chr1376779946414chr141714021999515chr151717362218716chr161925952827617chr172883483588818chr18604291017919chr193447753545120chr201099791238721chr2151720651822chr221329561272423chrX1503301761824chrY892467\n\n\n\nIn the next section we will use data from chr01 for comparative timings. It has the greatest number of intervals in both the reference and target groups."
  },
  {
    "objectID": "session2a-tables-and-arrow.html#reading-arrow-files-in-r",
    "href": "session2a-tables-and-arrow.html#reading-arrow-files-in-r",
    "title": "2a: Data Tables and Arrow files",
    "section": "Reading Arrow files in R",
    "text": "Reading Arrow files in R\n\nIn R (and in Python) the Arrow file format is confounded with an earlier file format called Feather and referred to as Feather V2.\nIn R the arrow::read_feather function returns a tibble. In an R session it looks like\n\n> library(tibble)\n> arrow::read_feather(\"biofast-data-v1/ex-rna.arrow\")\n# A tibble: 4,685,080 × 3\n   chromo     start      stop\n   <fct>      <int>     <int>\n 1 chr02  216499331 216501458\n 2 chr07  101239611 101245071\n 3 chr19   49487626  49491841\n 4 chr10   80155590  80169336\n 5 chr17   76270411  76271290\n 6 chr06   31268756  31272069\n 7 chr05  170083214 170083368\n 8 chr19   51989731  51989996\n 9 chr18   55225980  55226732\n10 chr16   84565611  84566066\n# … with 4,685,070 more rows\n\nThe RCall package in Julia allows for running an R process within a Julia session.\nOne way of executing R code with RCall is to prepend R to a string. This causes the string to be evaluated in R.\n$-interpolation in the string causes a Julia object to be copied into the R environment and its name in R interpolated.\n\n\nR\"\"\"\nlibrary(tibble)\nglimpse(rnatbl <- arrow::read_feather($rnaarrownm))\n\"\"\";\n\nRows: 4,685,080\nColumns: 3\n$ chromo <fct> chr02, chr07, chr19, chr10, chr17, chr06, chr05, chr19, chr18, …\n$ start  <int> 216499331, 101239611, 49487626, 80155590, 76270411, 31268756, 1…\n$ stop   <int> 216501458, 101245071, 49491841, 80169336, 76271290, 31272069, 1…"
  },
  {
    "objectID": "session2a-tables-and-arrow.html#reading-arrow-files-in-python",
    "href": "session2a-tables-and-arrow.html#reading-arrow-files-in-python",
    "title": "2a: Data Tables and Arrow files",
    "section": "Reading Arrow files in Python",
    "text": "Reading Arrow files in Python\n\nThe pyarrow package includes pyarrow.feather. Its use in a Python session looks like\n\n>>> import pyarrow.feather as fea\n>>> fea.read_table('./biofast-data-v1/ex-rna.arrow')\npyarrow.Table\nchromo: dictionary<values=string, indices=int8, ordered=0> not null\nstart: int32 not null\nstop: int32 not null\n----\nchromo: [  -- dictionary:\n[\"chr01\",\"chr02\",\"chr03\",\"chr04\",\"chr05\",...,\"chr20\",\"chr21\",\"chr22\",\"chrX\",\"chrY\"]  -- indices:\n[1,6,18,9,16,...,16,15,10,22,0]]\nstart: [[216499331,101239611,49487626,80155590,76270411,...,7014179,75627747,59636724,153785767,182839364]]\nstop: [[216501458,101245071,49491841,80169336,76271290,...,7014515,75631483,59666963,153787586,182887745]]\n>>> fea.read_feather('./biofast-data-v1/ex-rna.arrow')\n        chromo      start       stop\n0        chr02  216499331  216501458\n1        chr07  101239611  101245071\n2        chr19   49487626   49491841\n3        chr10   80155590   80169336\n4        chr17   76270411   76271290\n...        ...        ...        ...\n4685075  chr17    7014179    7014515\n4685076  chr16   75627747   75631483\n4685077  chr11   59636724   59666963\n4685078   chrX  153785767  153787586\n4685079  chr01  182839364  182887745\n\n[4685080 rows x 3 columns]\n\nread_table returns a Table object, read_feather returns a Pandas dataframe.\nThe PyCall package for Julia starts a Python process and allows communication with it, including data transfer.\nI use this instead of the Python REPL when working with both Julia and Python.\nConfiguring Python, Conda, pyarrow, pandas, and PyCall across platforms is sufficiently complicated to almost surely cause failures for some workshop participants. Instead of evaluating this code chunk we quote the results.\n\njulia> using PyCall\n\njulia> fea = pyimport(\"pyarrow.feather\");\n\njulia> fea.read_table(rnaarrownm)\nPyObject pyarrow.Table\nchromo: dictionary<values=string, indices=int8, ordered=0> not null\nstart: int32 not null\nstop: int32 not null\n----\nchromo: [  -- dictionary:\n[\"chr01\",\"chr02\",\"chr03\",\"chr04\",\"chr05\",...,\"chr20\",\"chr21\",\"chr22\",\"chrX\",\"chrY\"]  -- indices:\n[1,6,18,9,16,...,16,15,10,22,0]]\nstart: [[216499331,101239611,49487626,80155590,76270411,...,7014179,75627747,59636724,153785767,182839364]]\nstop: [[216501458,101245071,49491841,80169336,76271290,...,7014515,75631483,59666963,153787586,182887745]]"
  },
  {
    "objectID": "session2a-tables-and-arrow.html#version-information",
    "href": "session2a-tables-and-arrow.html#version-information",
    "title": "2a: Data Tables and Arrow files",
    "section": "Version information",
    "text": "Version information\n\nversioninfo()\n\nJulia Version 1.8.0-rc1\nCommit 6368fdc656 (2022-05-27 18:33 UTC)\nPlatform Info:\n  OS: Linux (x86_64-pc-linux-gnu)\n  CPU: 8 × 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz\n  WORD_SIZE: 64\n  LIBM: libopenlibm\n  LLVM: libLLVM-13.0.1 (ORCJIT, tigerlake)\n  Threads: 6 on 8 virtual cores"
  },
  {
    "objectID": "session2b-interval-overlap.html",
    "href": "session2b-interval-overlap.html",
    "title": "2b: Determining Interval Overlap",
    "section": "",
    "text": "Code\nusing Arrow          # Arrow storage and file format\nusing BenchmarkTools # tools for benchmarking code\nusing DataFrames     # versatile tabular data format\nusing IntervalTrees  # interval trees from BioJulia\nusing RangeTrees     # a bespoke implementation of interval trees\nusing Tables         # row- or column-oriented tabular data\n\nusing Base: intersect! # not exported from Base\n\ndatadir = joinpath(@__DIR__, \"biofast-data-v1\");"
  },
  {
    "objectID": "session2b-interval-overlap.html#creating-dictionaries-of-vectorunitrange",
    "href": "session2b-interval-overlap.html#creating-dictionaries-of-vectorunitrange",
    "title": "2b: Determining Interval Overlap",
    "section": "Creating dictionaries of Vector{UnitRange}",
    "text": "Creating dictionaries of Vector{UnitRange}\n\nA UnitRange, such as 2:10, includes the end points, accessed by first and last methods.\n\n\ntypeof(2:10), length(2:10), first(2:10), last(2:10)\n\n(UnitRange{Int64}, 9, 2, 10)\n\n\n\nHowever, the positions in the start and stop columns in a .bed file are not both included in the range they represent. The positions correspond to base pairs in the range start:(stop - 1) as 0-based indices on the chromosome or (start + 1):stop as 1-based indices.\nIt doesn’t matter which one we use as long as we are consistent in creating ranges from the reference intervals and the target intervals.\nWe choose to start counting from 1, just as the world’s foremost expert on counting does.\nWe wrap this conversion in a utility function to help ensure consistency.\n\n\nasrange(start, stop) = (start+one(start)):stop\n\nasrange (generic function with 1 method)\n\n\n\n\n\n\n\n\n“One-liner” function (actually method) definitions\n\n\n\n\n\nThis method definition uses the compact “one-liner” form, like the math notation f(x) = x + 1.\n\n\n\n\n\n\n\n\n\none(x) versus literal 1\n\n\n\n\n\none(x) is used instead of the literal 1 in asrange to preserve the integer type (see also ?oneunit, which is slighly more general).\n\nst = Int32(2314)\ntypeof(st + 1)       # type gets promoted to Int64\n\nInt64\n\n\n\ntypeof(st + one(st)) # type not promoted\n\nInt32\n\n\n\n\n\n\nCreate a utility, chromodict to read an Arrow.Table and convert it to a Dict{Symbol, Vector{UnitRange{T}}}, assuming that the table contains columns named :chromo, :start, and :stop.\nWe define two methods, one that takes a “row-table”, which is a vector of named tuples, and one that takes the file name of the arrow file, reads the (column oriented)table, converts it to a row table, and then calls the first method.\n\n\nfunction chromodict(rtbl::Vector{<:NamedTuple})\n  itype = promote_type(\n    Tables.columntype(rtbl, :start),\n    Tables.columntype(rtbl, :stop),\n  )\n  vtype = Vector{UnitRange{itype}}\n  dict = Dict{Symbol,vtype}()\n  for (; chromo, start, stop) in rtbl\n    push!(get!(dict, Symbol(chromo), vtype()), asrange(start, stop))\n  end\n  return dict\nend\nfunction chromodict(fnm::AbstractString)\n  return chromodict(rowtable(Arrow.Table(joinpath(datadir, fnm))))\nend\ntarrngvecs = chromodict(\"ex-rna.arrow\")\nrefrngvecs = chromodict(\"ex-anno.arrow\")\n\nDict{Symbol, Vector{UnitRange{Int32}}} with 24 entries:\n  :chr21 => [5011799:5011874, 5012548:5012687, 5014386:5014471, 5016935:5017145…\n  :chr15 => [19878555:19878668, 19878831:19879004, 19881201:19881307, 19882277:…\n  :chr10 => [14497:14604, 14061:14299, 16502:16544, 14138:14299, 44712:44901, 4…\n  :chr17 => [76723:76866, 75814:75878, 71366:71556, 65830:65887, 64099:65736, 1…\n  :chr07 => [12704:12822, 26965:27199, 24314:24365, 26965:27234, 31060:31194, 2…\n  :chr14 => [16057472:16057622, 18333726:18333900, 18337973:18338078, 18338243:…\n  :chr08 => [64269:64320, 64091:64175, 72601:72673, 78905:79775, 72617:72701, 7…\n  :chr12 => [12310:12358, 12740:12824, 13102:13201, 13370:13501, 31878:32015, 2…\n  :chr18 => [11103:11595, 15617:15822, 11191:11595, 13152:13354, 15617:15928, 1…\n  :chrX  => [253743:253846, 254937:255091, 276322:276394, 281482:281684, 284167…\n  :chr13 => [18177555:18178465, 18176018:18176170, 18174442:18174512, 18174010:…\n  :chr11 => [75780:76143, 86649:87586, 125578:125927, 121258:121426, 113116:113…\n  :chr22 => [10736171:10736283, 10961283:10961338, 10959067:10959136, 10950049:…\n  :chr03 => [23757:23812, 23968:24501, 54293:54346, 53348:53692, 196607:196859,…\n  :chr19 => [70928:70976, 66346:66499, 60951:61894, 62113:66524, 70928:70951, 6…\n  :chr05 => [58198:58915, 92151:92276, 113251:113448, 139483:140716, 143047:143…\n  :chr06 => [95124:95454, 105919:106856, 144536:144885, 140211:140379, 131910:1…\n  :chr20 => [87250:87359, 96005:97094, 87710:87767, 96005:96533, 142369:142686,…\n  :chrY  => [2784749:2784853, 2786855:2787699, 2789827:2790328, 2827982:2828218…\n  :chr04 => [49096:49956, 49554:50124, 53285:53491, 59430:59556, 60058:60153, 8…\n  :chr02 => [45440:46385, 38814:41627, 42809:42952, 41220:41627, 46807:46870, 4…\n  :chr01 => [11869:12227, 12613:12721, 13221:14409, 12010:12057, 12179:12227, 1…\n  :chr09 => [12134:12190, 12291:12340, 12726:12834, 13088:13157, 13338:13487, 1…\n  :chr16 => [11555:11908, 12294:12402, 12902:14090, 11861:11908, 12294:12378, 1…\n\n\n\n\n\n\n\n\nMutating get! for a Dict\n\n\n\n\n\nThe call get!(dict, Symbol(chromo), vtype()) in chromodict returns dict[Symbol(chromo)] or the default value, which is an empty Vector{UnitRange{T}}. For the case of the default, it also installs that key/value pair in dict.\n\n\n\n\n\n\n\n\n\nSymbols versus Strings for Dict keys\n\n\n\n\n\nWe use Symbols for the keys in these Dicts because they are easier to type and because symbol table lookup is very fast, although that doesn’t really matter when we only have 24 distinct keys.\n\n\n\n\n\n\n\n\n\nDestructuring a struct or NamedTuple\n\n\n\n\n\nThe expression for (; chromo, start, stop) in rtbl is equivalent to three local assignments\nfor r in rtbl\n  chromo = r.chromo\n  start = r.start\n  stop = r.stop\n  ...\nend\nIn other words, it is equivalent to taking the named fields of a NamedTuple or a struct and making them available in the local namespace under the same names.\n\n\n\n\n\n\n\n\n\nInterfaces to row- and column-oriented tables in Tables.jl\n\n\n\n\n\nTables.jl provides interfaces for row- and column-oriented tables, allowing for one orientation to be viewed as the other. In this case an Arrow.Table is column-oriented but rowtable of this table returns a Vector{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}} to iterate over the rows. When a “method instance” of chromodict is compiled for such a table, the schema of the rows will be known.\n\n@code_warntype chromodict(\n  rowtable(Arrow.Table(\"biofast-data-v1/ex-anno.arrow\")),\n)\n\nMethodInstance for chromodict(::\n\n\nVector{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}})\n  from chromodict(rtbl::Vector{<:NamedTuple}) in Main at In[7]:1\nArguments\n  #self#::Core.Const(chromodict)\n  rtbl::Vector{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}}\nLocals\n  @_3::Union{Nothing, Tuple{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}, Int64}}\n  dict::Dict{Symbol, Vector{UnitRange{Int32}}}\n  vtype::Type{Vector{UnitRange{Int32}}}\n  itype::Type{Int32}\n  stop::Int32\n  start::Int32\n  chromo::String\nBody::Dict{Symbol, Vector{UnitRange{Int32}}}\n\n\n1 ─ %1  = Tables.columntype\n\n\n::Core.Const(Tables.columntype)\n│   %2  = (\n\n\n%1)(rtbl, :start)::Core.Const(Int32)\n│   %3  = Tables.columntype::Core.Const(Tables.columntype)\n│   %4  = (%3)(rtbl, :stop)::Core.Const(Int32)\n│         (itype = Main.promote_type(%2, %4))\n│   %6  = Core.apply_type(Main.UnitRange, itype\n\n\n::Core.Const(Int32))::Core.Const(UnitRange{Int32})\n│         (vtype = Core.apply_type(Main.Vector, %6))\n│   %8  = Core.apply_type(Main.Dict, Main.Symbol, vtype::Core.Const(Vector{UnitRange{Int32}}))::Core.Const(Dict{Symbol, Vector{UnitRange{Int32}}})\n│         (dict = (%8)())\n│   %10 = rtbl::Vector{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}}\n│         (@_3 = Base.iterate(%10))\n│   %12 = (@_3 === nothing)::Bool\n│   %13 = Base.not_int(%12)::Bool\n└──       goto #4 if not %13\n2 ┄ %15 = @_3::Tuple{NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}, Int64}\n│   %16 = Core.getfield(%15, 1)::NamedTuple{(:chromo, :start, :stop), Tuple{String, Int32, Int32}}\n│         (chromo = Base.getproperty(%16, :chromo))\n│         (start = Base.getproperty(%16, :start))\n│         (stop = Base.getproperty(%16, :stop))\n│   %20 = Core.getfield(%15, 2)::Int64\n│   %21 = dict::Dict{Symbol, Vector{UnitRange{Int32}}}\n│   %22 = Main.Symbol(chromo)::Symbol\n│   %23 = (vtype::Core.Const(Vector{UnitRange{Int32}}))()::Vector{UnitRange{Int32}}\n│   %24 = Main.get!(%21, %22, %23)::Vector{UnitRange{Int32}}\n│   %25 = Main.asrange(start, stop)::UnitRange{Int32}\n│         Main.push!(%24, %25)\n│         (@_3 = Base.iterate(%10, %20))\n│   %28 = (@_3 === nothing)::Bool\n│   %29 = Base.not_int(%28)::Bool\n└──       goto #4 if not %29\n3 ─       goto #2\n4 ┄       return dict\n\n\n\n\n\n\n\nIn refrngvecs, each of the values, a Vector{UnitRange}, should be sorted by the first element of the UnitRange.\nCreating an IntervalTree requires the ranges to be sorted by first element and by last element when the first elements are equal.\nDefine a custom lt comparison for this.\n\n\nlet\n  function lt(x::UnitRange{T}, y::UnitRange{T}) where {T}\n    fx, fy = first(x), first(y)\n    return fx == fy ? last(x) < last(y) : fx < fy\n  end\n  for v in values(refrngvecs)\n    sort!(v; lt)\n  end\nend\nrefrngvecs  # note changes in refrngvecs[:chr01]\n\nDict{Symbol, Vector{UnitRange{Int32}}} with 24 entries:\n  :chr21 => [5011799:5011874, 5012548:5012687, 5014386:5014471, 5016935:5017145…\n  :chr15 => [19878555:19878668, 19878831:19879004, 19881201:19881307, 19882277:…\n  :chr10 => [14061:14299, 14138:14299, 14497:14604, 16502:16544, 44712:44901, 4…\n  :chr17 => [64099:65736, 65830:65887, 71366:71556, 75814:75878, 76723:76866, 8…\n  :chr07 => [12704:12822, 19018:19172, 19619:19895, 20834:21029, 24314:24365, 2…\n  :chr14 => [16057472:16057622, 18333726:18333900, 18333826:18333896, 18337973:…\n  :chr08 => [64091:64175, 64269:64320, 72601:72673, 72617:72701, 78905:79244, 7…\n  :chr12 => [12310:12358, 12740:12824, 13102:13201, 13370:13501, 14522:14944, 1…\n  :chr18 => [11103:11595, 11191:11595, 13152:13354, 14195:14653, 14490:14653, 1…\n  :chrX  => [253743:253846, 254937:255091, 276322:276394, 276324:276394, 276353…\n  :chr13 => [18174010:18174103, 18174442:18174512, 18176018:18176170, 18177555:…\n  :chr11 => [75780:76143, 86649:87586, 112967:113111, 113116:113174, 121258:121…\n  :chr22 => [10736171:10736283, 10939388:10939423, 10940597:10940707, 10941691:…\n  :chr03 => [23757:23812, 23968:24501, 53348:53692, 54293:54346, 195758:195914,…\n  :chr19 => [60951:61894, 62113:66524, 63821:64213, 65051:65226, 65822:66047, 6…\n  :chr05 => [58198:58915, 92151:92276, 113251:113448, 139483:140716, 140258:140…\n  :chr06 => [95124:95454, 105919:106856, 131910:132117, 140211:140379, 142272:1…\n  :chr20 => [87250:87359, 87710:87767, 96005:96533, 96005:97094, 142369:142686,…\n  :chrY  => [2784749:2784853, 2786855:2787699, 2789827:2790328, 2827982:2828218…\n  :chr04 => [49096:49956, 49554:50124, 53285:53491, 53286:53491, 53295:53491, 5…\n  :chr02 => [38814:41627, 41220:41627, 41221:41627, 42809:42952, 45440:46385, 4…\n  :chr01 => [11869:12227, 12010:12057, 12179:12227, 12613:12697, 12613:12721, 1…\n  :chr09 => [12134:12190, 12291:12340, 12726:12834, 13088:13157, 13338:13487, 1…\n  :chr16 => [11555:11908, 11861:11908, 12294:12378, 12294:12402, 12663:12733, 1…\n\n\n\n\n\n\n\n\nlet blocks\n\n\n\n\n\nA let/end block provides a local namespace. The custom lt comparison method will not be visible outside the scope of the block.\n\n\n\n\nWe will use the ranges on chromsome 1 for our timing benchmarks. The target for tests of intersection with a single target range will be the last range on chromosome 1 in “ex-rna.arrow”.\n\n\nrefrngvec01 = refrngvecs[:chr01]\ntarrngvec01 = tarrngvecs[:chr01]\ntarget = last(tarrngvec01)\n\n182839365:182887745"
  },
  {
    "objectID": "session2b-interval-overlap.html#rangetrees",
    "href": "session2b-interval-overlap.html#rangetrees",
    "title": "2b: Determining Interval Overlap",
    "section": "RangeTrees",
    "text": "RangeTrees\n\nRangeTrees.jl provides an implementation of interval trees using the augmented binary tree formulation.\nBecause the tree is represented by its root node, there is no RangeTree type or constructor, only a RangeNode.\n\n\nrefrngtrees = Dict(k => RangeNode(v) for (k, v) in refrngvecs)\nrangetree01 = refrngtrees[:chr01]\ntreesize(rangetree01), treeheight(rangetree01), treebreadth(rangetree01)\n\n(52789, 15, 20022)\n\n\n\nprint_tree(rangetree01; maxdepth = 3)\n\n(110172080:110172489, 248937043)\n├─ (37858488:37858539, 110172217)\n│  ├─ (19100078:19100126, 37857709)\n│  │  ├─ (7745845:7746091, 19099677)\n│  │  │  ⋮\n│  │  │  \n│  │  └─ (27615689:27615844, 37857709)\n│  │     ⋮\n│  │     \n│  └─ (62834038:62834116, 110172217)\n│     ├─ (46194582:46194651, 62829176)\n│     │  ⋮\n│     │  \n│     └─ (89633140:89633322, 110172217)\n│        ⋮\n│        \n└─ (169855796:169855957, 248937043)\n   ├─ (153545753:153545802, 169854964)\n   │  ├─ (145901516:145901664, 153545518)\n   │  │  ⋮\n   │  │  \n   │  └─ (157519743:157519770, 169854964)\n   │     ⋮\n   │     \n   └─ (208028830:208030303, 248937043)\n      ├─ (196793317:196793633, 208029042)\n      │  ⋮\n      │  \n      └─ (228147615:228147699, 248937043)\n         ⋮\n         \n\n\n\n\n\n\n\n\nAugmented interval trees\n\n\n\n\n\nA smaller example may help to understand how this type of interval tree. Consider the first 7 UnitRanges in refrngvecs[:chr01]\n\nsmallrngvec = refrngvecs[:chr01][1:7]\n\n7-element Vector{UnitRange{Int32}}:\n 11869:12227\n 12010:12057\n 12179:12227\n 12613:12697\n 12613:12721\n 12975:13052\n 13221:13374\n\n\n\nrn = RangeNode(smallrngvec)\nprint_tree(rn)\n\n(12613:12697, 13374)\n├─ (12010:12057, 12227)\n│  ├─ (11869:12227, 12227)\n│  └─ (12179:12227, 12227)\n└─ (12975:13052, 13374)\n   ├─ (12613:12721, 12721)\n   └─ (13221:13374, 13374)\n\n\n\nThe UnitRange in the root node is the 4th out of the 7 sorted ranges from which the tree was constructed.\nEach of the nodes in the tree can have 0, 1, or 2 child nodes. Those with 0 children are called the “leaves” of the tree.\n\n\ncollect(Leaves(rn))\n\n4-element Vector{RangeNode{Int32, Int32}}:\n (11869:12227, 12227)\n (12179:12227, 12227)\n (12613:12721, 12721)\n (13221:13374, 13374)\n\n\n\nIn addition to the UnitRange it represents, each RangeNode stores maxlast, the maximum value of last(rng) for any UnitRange in the tree rooted at this node. For a leaf maxlast is simply last of its UnitRange.\nFor other nodes, maxlast can be larger than last of its UnitRange.\nIn particular, for the root node maxlast is the maximum of all the last values of the UnitRanges that generated the tree.\n\n\nmaximum(last.(smallrngvec))\n\n13374\n\n\nRangeTrees.jl defines methods for generics like children, getroot, and nodevalue from AbstractTrees.jl and these allow for many other generics to be applied to a RangeNode.\n\nchildren(rangetree01)\n\n2-element Vector{RangeNode{Int32, Int32}}:\n (37858488:37858539, 110172217)\n (169855796:169855957, 248937043)\n\n\nThe root of the tree can be obtained from any node using getroot. The combination of getroot and children allows traversal of the tree.\n\ngetroot(first(children(first(children(rangetree01))))) # get the root from its grandchild\n\n(110172080:110172489, 248937043)\n\n\n\n\n\n\nEvaluating and storing maxlast in the nodes allows for overlap searchs to be truncated at nodes for which maxlast < first(target). The sorting by first allows for skipping the right subtree whenever last(target) < first(thisnode) (as in the intersect! method for Vector{UnitRange}).\nintersect and intersect! methods are already defined in RangeTrees.jl.\nCheck that their results agree with the saved result.\n\n\nisequal(intersect(target, rangetree01), savedresult)\n\ntrue\n\n\n\nisequal(intersect!(result, target, rangetree01), savedresult)\n\ntrue"
  },
  {
    "objectID": "session2b-interval-overlap.html#intervaltrees",
    "href": "session2b-interval-overlap.html#intervaltrees",
    "title": "2b: Determining Interval Overlap",
    "section": "IntervalTrees",
    "text": "IntervalTrees\n\nCreate a dictionary of IntervalTrees. It is somewhat tedious to get the type of the result correct and we create a function to hide the details.\n\n\nfunction toitrees(rngdict::Dict{S,Vector{UnitRange{T}}}) where {S,T}\n  return Dict(\n    k => IntervalTree{T,Interval{T}}(Interval.(v)) for (k, v) in rngdict\n  )\nend\nrefintvltrees = toitrees(refrngvecs)\nintvltree01 = refintvltrees[:chr01]\nshow(intvltree01)\n\nIntervalTree{Int32, Interval{Int32}}\n(11869,12227)\n(12010,12057)\n(12179,12227)\n⋮\n(248917279,248917401)\n(248917279,248919946)\n(248936581,248937043)\n\n\n\nCreating an intersect! method is also tedious because the package has its own Interval data type and defines intersect(itr::IntervalTree, (frst, lst)) to return an iterator of Intervals in the tree, not the intersection\n\n\nInterval(target)\n\nInterval{Int32}\n(182839365,182887745)\n\n\n\nfunction Base.intersect!(\n  res::Vector{UnitRange{T}},\n  target::AbstractUnitRange,\n  refs::IntervalTree{T},\n) where {T}\n  empty!(res)\n  firstt, lastt = first(target), last(target)\n  for isect in intersect(refs, (firstt, lastt))\n    push!(res, max(first(isect), firstt):min(last(isect), lastt))\n  end\n  return res\nend\nisequal(intersect!(result, target, intvltree01), savedresult)\n\ntrue"
  },
  {
    "objectID": "session3-linear-mixed-effects.html",
    "href": "session3-linear-mixed-effects.html",
    "title": "3a: Linear Mixed-effects Models",
    "section": "",
    "text": "\\[\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\]\nAttach the packages to be used"
  },
  {
    "objectID": "session4-exercise.html",
    "href": "session4-exercise.html",
    "title": "4: Hands-on exercise",
    "section": "",
    "text": "Instructions: Participants will translate their R/python script to Julia with assistance from the workshop presenters and TAs. After the 60 minutes, one or two volunteers will share their script to the group.\n\nChecklist\n\nDo you have Julia installed? If not, go to the getting started notes\nDo you have your project folder and installed dependencies? If not, go to the getting started nodes\nDo you have your data in your project folder?\nHave you selected an editor to use (VSCode, Emacs, Jupyter notebooks,…)? Check out the available options in the Julia website\n\nYou are ready to start writing Julia code! You can start with a simple .jl file, or you could try with a jupyter notebook or a .qmd file."
  },
  {
    "objectID": "session5-other-tools.html",
    "href": "session5-other-tools.html",
    "title": "5: Other tools for Data Science",
    "section": "",
    "text": "Data tools with Arrow.jl and Tables.jl\nModel fitting with MixedModels.jl"
  },
  {
    "objectID": "session5-other-tools.html#communication-with-other-systems-julia-interoperability",
    "href": "session5-other-tools.html#communication-with-other-systems-julia-interoperability",
    "title": "5: Other tools for Data Science",
    "section": "Communication with other systems: Julia interoperability",
    "text": "Communication with other systems: Julia interoperability\nJuliaInterop\n\nNote: Both RCall and PyCall are written 100% julia\n\nRCall\nDocumentation\nSwitching between julia and R using $:\njulia> using RCall\n\njulia> foo = 1\n1\n\nR> x <- $foo\n\nR> x\n[1] 1\nMacros @rget and @rput:\njulia> z = 1\n1\n\njulia> @rput z\n1\n\nR> z\n[1] 1\n\nR> r = 2\n\njulia> @rget r\n2.0\n\njulia> r\n2.0\nR\"\" string macro:\njulia> R\"rnorm(10)\"\nRObject{RealSxp}\n [1]  0.9515526 -2.1268329 -1.1197652 -1.3737837 -0.5308834 -0.1053615\n [7]  1.0949319 -0.8180752  0.7316163 -1.3735100\nLarge chunk of code:\njulia> y=1\n1\n\njulia> R\"\"\"\n       f<-function(x,y) x+y\n       ret<- f(1,$y)\n       \"\"\"\nRObject{RealSxp}\n[1] 2\n\nA small example from this blog\nSimulate data\njulia> using Random\n\njulia> Random.seed!(1234)\nMersenneTwister(1234)\n\njulia> X = randn(3,2)\n3×2 Matrix{Float64}:\n  0.867347  -0.902914\n -0.901744   0.864401\n -0.494479   2.21188\n\njulia> b = reshape([2.0, 3.0], 2,1)\n2×1 Matrix{Float64}:\n 2.0\n 3.0\n\njulia> y = X * b + randn(3,1)\n3×1 Matrix{Float64}:\n -0.4412351955236954\n  0.5179809120122916\n  6.149009488103242\nFit a model\njulia> @rput y\n3×1 Matrix{Float64}:\n -0.4412351955236954\n  0.5179809120122916\n  6.149009488103242\n\njulia> @rput X\n3×2 Matrix{Float64}:\n  0.867347  -0.902914\n -0.901744   0.864401\n -0.494479   2.21188\n\njulia> R\"mod <- lm(y ~ X-1)\"\nRObject{VecSxp}\n\nCall:\nlm(formula = y ~ X - 1)\n\nCoefficients:\n   X1     X2  \n2.867  3.418 \n\njulia> R\"summary(mod)\"\nRObject{VecSxp}\n\nCall:\nlm(formula = y ~ X - 1)\n\nResiduals:\n       1        2        3 \n0.158301 0.148692 0.006511 \n\nCoefficients:\n   Estimate Std. Error t value Pr(>|t|)  \nX1   2.8669     0.2566   11.17   0.0568 .\nX2   3.4180     0.1359   25.15   0.0253 *\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 0.2173 on 1 degrees of freedom\nMultiple R-squared:  0.9988,    Adjusted R-squared:  0.9963 \nF-statistic: 404.8 on 2 and 1 DF,  p-value: 0.03512\n\njulia> R\"plot(X[,1],y)\"\n\n\n\nPyCall\nDocumentation\nNote that (@v1.8) pkg> add PyCall will use the Conda.jl package to install a minimal Python distribution (via Miniforge) that is private to Julia (not in your PATH).\nWe need to make sure that which conda points at the conda folder inside .julia, so we need to put ~/.julia/conda/3/bin early on the PATH. In Mac zsh, we need to add export PATH=~/.julia/conda/3/bin:$PATH in the ~/.zshrc file. (Those who prefer not to conda-ize their entire environment may instead choose just to link ~/.julia/conda/3/bin/{conda,jupyter,python,python3} somewhere on their existing path, such as ~/bin.)\nSimple example:\nusing PyCall\nmath = pyimport(\"math\")\nmath.sin(math.pi / 4)\npy\"...\" evaluates \"...\" as Python code:\npy\"\"\"\nimport numpy as np\n\ndef sinpi(x):\n    return np.sin(np.pi * x)\n\"\"\"\npy\"sinpi\"(1)\n\n\nMore on Julia/python connectivity\n\nThe pyjulia module allows you to call Julia directly from Python\nCheck out the packages in JuliaPy"
  },
  {
    "objectID": "session5-other-tools.html#package-system",
    "href": "session5-other-tools.html#package-system",
    "title": "5: Other tools for Data Science",
    "section": "Package system",
    "text": "Package system\n\nStarting on Julia 1.6, precompilation is much faster\nMany changes under the hood that allow things to work faster and more smoothly\nA local environment can be established and preserved with Project.toml and Manifest.toml files.\nUse of Artifacts.toml allows for binary dependencies\n\n\nLandscape of Julia packages for biology\n\nBioJulia is a combination of Julia packages for biology applications.\nJulia for Biologists is an arxiv paper the features that make Julia a perfect language for bioinformatics and computational biology.\nList of useful packages by SMLP2022"
  },
  {
    "objectID": "session5-other-tools.html#plotting",
    "href": "session5-other-tools.html#plotting",
    "title": "5: Other tools for Data Science",
    "section": "Plotting",
    "text": "Plotting\n\nMakie ecosystem\nPlots.jl\nOther graphics packages available in Julia"
  },
  {
    "objectID": "session5-other-tools.html#performance-tips",
    "href": "session5-other-tools.html#performance-tips",
    "title": "5: Other tools for Data Science",
    "section": "Performance tips",
    "text": "Performance tips\nSee more in Julia docs\n\n@time to measure performance\njulia> x = rand(1000);\n\njulia> function sum_global()\n           s = 0.0\n           for i in x\n               s += i\n           end\n           return s\n       end;\n\njulia> @time sum_global()  ## function gets compiled\n  0.017705 seconds (15.28 k allocations: 694.484 KiB)\n496.84883432553846\n\njulia> @time sum_global()\n  0.000140 seconds (3.49 k allocations: 70.313 KiB)\n496.84883432553846\n\n\nBreak functions into multiple definitions\nThe function\nusing LinearAlgebra\n\nfunction mynorm(A)\n    if isa(A, Vector)\n        return sqrt(real(dot(A,A)))\n    elseif isa(A, Matrix)\n        return maximum(svdvals(A))\n    else\n        error(\"mynorm: invalid argument\")\n    end\nend\nshould really be written as\nnorm(x::Vector) = sqrt(real(dot(x, x)))\nnorm(A::Matrix) = maximum(svdvals(A))\nto allow the compiler to directly call the most applicable code.\n\nMultiple dispatch\n\nThe choice of which method to execute when a function is applied is called dispatch\nJulia allows the dispatch process to choose based on the number of arguments given, and on the types of all of the function’s arguments\nThis is denoted multiple dispatch\nThis is different than traditional object-oriented languages, where dispatch occurs based only on the first argument\n\njulia> f(x::Float64, y::Float64) = 2x + y\nf (generic function with 1 method)\n\njulia> f(2.0, 3.0)\n7.0\n\njulia> f(2.0, 3)\nERROR: MethodError: no method matching f(::Float64, ::Int64)\nClosest candidates are:\n  f(::Float64, !Matched::Float64) at none:1\nCompare to\njulia> f(x::Number, y::Number) = 2x + y\nf (generic function with 2 methods)\n\njulia> f(2.0, 3.0)\n7.0\n\njulia> f(2, 3.0)\n7.0\n\njulia> f(2.0, 3)\n7.0\n\njulia> f(2, 3)\n7\n\n\n\nProfiling\nRead more in Julia docs.\njulia> function myfunc()\n           A = rand(200, 200, 400)\n           maximum(A)\n       end\n\njulia> myfunc() # run once to force compilation\n\njulia> using Profile\n\njulia> @profile myfunc()\n\njulia> Profile.print()\nTo see the profiling results, there are several graphical browsers (see Julia docs).\n\n\nOther packages for performance\n\nBenchmarkTools.jl: performance tracking of Julia code\nTraceur.jl: You run your code, it tells you about any obvious performance traps"
  },
  {
    "objectID": "session5-other-tools.html#literate-programming",
    "href": "session5-other-tools.html#literate-programming",
    "title": "5: Other tools for Data Science",
    "section": "Literate programming",
    "text": "Literate programming\n\nquarto.org. These notes are rendered with quarto!\nJupyter\nPluto.jl\nWeave.jl package provides “Julia markdown” and also provides support for converting between jmd files and Jupyter notebooks.\nLiterate.jl is a simple package for literate programming (i.e. programming where documentation and code are “woven” together) and can generate Markdown, plain code and Jupyter notebook output.\nDocumenter.jl is the standard tool for building webpages from Julia documentation\nBooks.jl is a package designed to offer somewhat similar functionality to the bookdown package in R."
  },
  {
    "objectID": "session6-conclusions.html",
    "href": "session6-conclusions.html",
    "title": "6: Conclusions",
    "section": "",
    "text": "Tutorial feedback form\nPlease don’t forget to fill out the ISMB/ECCB 2022 Tutorial Feedback."
  },
  {
    "objectID": "session3-linear-mixed-effects.html#data-sets-to-be-used-in-examples",
    "href": "session3-linear-mixed-effects.html#data-sets-to-be-used-in-examples",
    "title": "3: Linear Mixed-effects Models",
    "section": "Data sets to be used in examples",
    "text": "Data sets to be used in examples\n\nThe sleepstudy data\nFor example, the sleepstudy dataset in the lme4 package for R is from a study on the effect of sleep deprivation on reaction time. A sample from the population of interest (long-distance truck drivers) had their average response time measured when they were on their regular sleep schedule and after one up to nine days of sleep deprivation (allowed only 3 hours per day in which to sleep).\n\n\n\n\n\n\nThis data description is inaccurate\n\n\n\n\n\nThe description of these data is inaccurate. Unfortunately by the time we learned this the researchers were no longer able to locate the original data. We keep using this example because it is such a nice example, even if the description is not quite accurate.\n\n\n\n\n\nCode\nusing CairoMakie        # graphics backend\nusing DataFrames\nusing LinearAlgebra\nusing MixedModels\nusing MixedModelsMakie  # special graphics for mixed models\nusing ProgressMeter     # report iteration speed when fitting models\nusing RCall\n\nif !isdefined(Main, :contrasts)\n  const contrasts = Dict{Symbol,Any}()  # to hold specific contrast declarations\nend\nCairoMakie.activate!(; type=\"svg\")\nProgressMeter.ijulia_behavior(:clear);\n\n\n┌ Info: Precompiling MixedModels [ff71e718-51f3-5ec2-a782-8ffcbfa3c316]\n└ @ Base loading.jl:1515\n\n\n┌ Info: Precompiling MixedModelsMakie [b12ae82c-6730-437f-aff9-d2c38332a376]\n└ @ Base loading.jl:1515\n\n\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"), width=7, height=5);\nR\"\"\"\nprint(\n  lattice::xyplot(\n    reaction ~ days | subj,\n    $(DataFrame(sleepstudy)),\n    type = c(\"g\",\"p\",\"r\"), layout = c(9,2),\n    index = function(x,y) coef(lm(y ~ x))[1],\n    xlab = \"Days of sleep deprivation\",\n    ylab = \"Average reaction time (ms)\",\n    aspect = \"xy\"\n  )\n)\n\"\"\";\n\n\n\n\n\nFigure 1: Average reaction time [ms.] versus days of sleep deprivation by participant. The panels are ordered according to increasing initial reaction time starting at the lower left.\n\n\n\n\n\nEach panel shows the data from one subject as well as a simple linear regression line fit to that subject’s data only.\nThe panels are ordered by increasing intercept of the within-subject line row-wise, starting at the bottom left.\nSome subjects, e.g. 310 and 309, have fast reaction times and are almost unaffected by the sleep deprivation.\nOthers, e.g. 337, start with slow reaction times which then increase substantially after sleep depreivation.\nA suitable model for these data would include an intercept and slope for the “typical” subject and randomly distributed deviations from these values for each of the observed subjects.\nThe assumed distribution of the random effects vector is multivariate Gaussian with mean zero (because they represent deviations from the population parameters) and an unknown covariance matrix, \\(\\bbSigma\\), to be estimated from the data.\nBecause \\(\\bbSigma\\) is a covariance matrix it must be symmetric and be positive-definite, a condition that is similar to the requirement that a scalar variance must be positive.\nIn particular, a positive-definite matrix like \\(\\bbSigma\\) has a “square root” in the sense that there is a matrix \\(\\bbL\\) such that \\(\\bbSigma=\\bbL\\bbL^\\prime\\). (Multiplying by \\(\\bbL^\\prime\\) instead of squaring \\(\\bbL\\) is necessary to ensure that the product is symmetric.)\nIn fact, there are several such matrices \\(\\bbL\\). If we require that \\(\\bbL\\) is lower triangular and that its diagonal entries be positive, there is only one such matrix, which is called the lower (or left) Cholesky factor.\nAs shown later, some of the expressions for the likelihood can be simplified if any scale parameter in the distribution of the response, given the random effects, is incorporated into the covariance matrix of the random effects.\nDefine \\(\\bblambda\\) to be the lower triangular matrix with non-negative diagonal entries such that\n\n\\[\n\\bbSigma=\\sigma^2{\\bblambda}{\\bblambda}^\\prime\n\\]\n\nFit a model with fixed effects for the intercept and the slope with respect to days of sleep deprivation and, possibly correlated, random effects for each of these coefficients by Subject.\n\n\ncontrasts[:subj] = Grouping()\nm1 = let\n  form = @formula reaction ~ 1 + days + (1+days|subj);\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\n\nMinimizing 58    Time: 0:00:00 ( 6.07 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting “contrasts”\n\n\n\n\n\nAs in R, the name contrasts is used in statistical modeling packages for Julia is the general sense of “What should be done with this categorical covariate?” Here it is specified that the :subj covariate will be used as a grouping factor for the random effects.\nIt is not particularly important when there are 18 levels, as is the case here, but when there are thousands or tens of thousands of levels it is very important to specify this.\n\n\n\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182\n\n\n\n\nThe “estimated” random effects from this model are eighteen vectors, one for each subject, and each of length two (deviation for the intercept and for the slope). These are returned as a \\(2\\times 18\\) matrix.\n\nfirst(ranef(m1))\n\n2×18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   …  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThe fixed-effects coefficients are the typical values for the population - initial reaction time of about 250 ms. and about 10.5 ms. increase in reaction time per day of sleep deprivation.\n\nshow(fixef(m1))\n\n[251.40510484848375, 10.467285959595792]\n\n\nFor this model the matrix \\(\\bblambda\\) is estimated as\n\nλ = first(m1.λ)\n\n2×2 LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\nand the (maximum likelihood) estimate of \\(\\sigma^2\\), as shown in the “Variance components” table, is\n\nσ² = varest(m1)\n\n654.9414503759484\n\n\nThus the (maximum likelihood) estimate of the covariance matrix \\(\\bbSigma\\) is\n\nΣ = σ² * λ * λ'\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nThe correlation shown in the “Variance components” table can be evaluated as\n\nΣ[2,1] / sqrt(Σ[1,1] * Σ[2,2])\n\n0.08133216669497845"
  },
  {
    "objectID": "session3-linear-mixed-effects.html#the-big-picture",
    "href": "session3-linear-mixed-effects.html#the-big-picture",
    "title": "3: Linear Mixed-effects Models",
    "section": "The Big Picture",
    "text": "The Big Picture\nAlthough it is tempting to construct the model on a per-subject basis it is ultimately easier to consider the entire set of responses and the collection of all of the random effects together. There are two reasons for this. First, the parameters must be estimated from the complete data set. Second, in situations where there is more than one grouping factor for the random effects it may not be possible to partition the responses according to the grouping factor. In the sleepstudy example the 180 observations can be partitioned into eighteen groups of ten observations on each of the eighteen subjects. However, in an example we will consider below each observation is on one of 56 subjects and one of 32 items and those classifications are crossed. That is, each subject is tested on each item and each item is tested on each subject. (Well, that was the plan at least. As often happens a few observations were erroneously recorded so the factors are not completely crossed in the data after cleaning.)\nIn any case, we write \\(\\bbb\\) for the complete random-effects vector (in this case a 36-dimensional vector formed from the \\(2\\times 18\\) matrix in column-major order).\n\nb = vec(first(ranef(m1)))\n\n36-element Vector{Float64}:\n   2.8158191325913045\n   9.07551171690184\n -40.04844170604461\n  -8.644079452057246\n -38.43306364816205\n  -5.513398016839489\n  22.832111878347245\n  -4.658717369983344\n  21.549840238279756\n  -2.944492887363264\n   8.815541327624107\n  -0.2352007017257725\n  16.441907752708154\n   ⋮\n   4.273998155877379\n  -2.955331793885193\n  20.622181380270188\n   3.561712799915034\n   3.2585352271136094\n   0.8717108026867305\n -24.710141525318363\n   4.659700898746931\n   0.7232620377478233\n  -0.9710526342424729\n  12.118907799956531\n   1.31069806890679\n\n\nIn the model the unconditional distribution of the random variable \\(\\mathcal{B}\\) is\n\\[\n\\mathcal{B}\\sim\\mathcal{N}\\left(\\mathbf{0},\\sigma^2\\bbLambda\\bbLambda^\\prime\\right)\n\\]\nand the conditional distribution of the response vector, \\(\\mathcal{Y}\\), is\n\\[\n(\\mathcal{Y}|\\mathcal{B}=\\bbb)\\sim\\mathcal{N}\\left(\\mathbf{X\\bbbeta}+\\mathbf{Zb}, \\sigma^2\\mathbf{I}_n\\right)\n\\]\nThe model matrix \\(\\mathbf{X}\\) for the fixed-effects has the usual form\n\nInt.(m1.X)  # display as Int to reduce clutter\n\n180×2 Matrix{Int64}:\n 1  0\n 1  1\n 1  2\n 1  3\n 1  4\n 1  5\n 1  6\n 1  7\n 1  8\n 1  9\n 1  0\n 1  1\n 1  2\n ⋮  \n 1  8\n 1  9\n 1  0\n 1  1\n 1  2\n 1  3\n 1  4\n 1  5\n 1  6\n 1  7\n 1  8\n 1  9\n\n\nbut the model matrix \\(\\mathbf{Z}\\) for the random effects is very sparse. That is, most of the entries in \\(\\mathbf{Z}\\) are zero.\n\nInt.(first(m1.reterms))\n\n180×36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nIn practice \\(\\mathbf{Z}\\) is stored and manipulated as a special type of sparse matrix.\nThe matrix \\(\\Lambda\\) is block-diagonal consisting of 18 diagonal blocks of size \\(2\\times 2\\), each of which is a copy of \\(\\lambda\\). It could be written as a Kronecker product\n\nΛ = kron(I(18), first(m1.λ))\n\n36×36 Matrix{Float64}:\n 0.929221   0.0       0.0        0.0       …  0.0       0.0        0.0\n 0.0181684  0.222645  0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.929221   0.0          0.0       0.0        0.0\n 0.0        0.0       0.0181684  0.222645     0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0       …  0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0       …  0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n ⋮                                         ⋱                       ⋮\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0       …  0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0       …  0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.0        0.0\n 0.0        0.0       0.0        0.0          0.222645  0.0        0.0\n 0.0        0.0       0.0        0.0          0.0       0.929221   0.0\n 0.0        0.0       0.0        0.0       …  0.0       0.0181684  0.222645\n\n\nbut there is no need to actually construct \\(\\bbLambda\\). It is completely determined by \\(\\bblambda\\)."
  },
  {
    "objectID": "session3-linear-mixed-effects.html#spherical-random-effects",
    "href": "session3-linear-mixed-effects.html#spherical-random-effects",
    "title": "3: Linear Mixed-effects Models",
    "section": "Spherical random effects",
    "text": "Spherical random effects\nOne of the many useful properties of the normal distribution is that a scalar normal distribution, \\(\\mathcal{X}\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), can be expressed in terms of the standard normal distribution, \\(\\mathcal{Z}\\sim\\mathcal{N}(0,1)\\) as\n\\[\n\\mathcal{X} = \\mu + \\sigma \\mathcal{Z}\n\\]\nA similar result holds for the multivariate normal distribution. The random effects vector, \\(\\mathcal{B}\\), with distribution \\(\\mathcal{N}(\\mathbf{0},\\bbSigma)\\) can be generated from a “spherical” random effects vector, \\(\\mathcal{U}\\), as\n\\[\n\\mathcal{B} = \\Lambda \\mathcal{U}\\quad\\mathrm{where}\\quad\\mathcal{U}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^2\\mathbf{I}_q)\n\\]\nand \\(q\\) is the dimension of the random-effects vector (36 in our example).\n(Recall that a multivariate normal distribution with covariance matrix \\(\\sigma^2\\mathbf{I}\\) is called a “spherical normal” because the contours of constant probability density are spheres. The random effects vector \\(\\mathcal{U}\\) has such a spherical distribution.)\nNow the conditional distribution of the response, given the random effects, can be written in terms of \\(\\mathcal{U}\\) as\n\\[\n(\\mathcal{Y}|\\mathcal{U}=\\mathbf{u})\\sim\\mathcal{N}\\left(\\mathbf{X\\bbbeta}+\\mathbf{Z\\Lambda u}, \\sigma^2\\mathbf{I}_n\\right)\n\\]\nThe joint probability density for \\(\\mathcal{Y}\\) and \\(\\mathcal{U}\\) is the product of the conditional density of \\(\\mathcal{Y}|\\mathcal{U}=\\mathbf{u}\\) and the unconditional density of \\(\\mathcal{U}\\).\n\\[\n\\begin{aligned}\nf_{\\mathcal{Y},\\mathcal{U}}(\\mathbf{y},\\mathbf{u})&= \\frac{1}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\\frac{\\|\\mathbf{y}-\\mathbf{X\\bbbeta}-\\mathbf{Z\\Lambda u}\\|^2}{2\\sigma^2}\\right)\\,\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\\exp\\left(-\\frac{\\|\\mathbf{u}\\|^2}{2\\sigma^2}\\right)\\\\\n&=\\frac{1}{(2\\pi\\sigma^2)^{(n+q)/2}}\\exp\\left(-\\frac{\\|\\mathbf{y}-\\mathbf{X\\bbbeta}-\\mathbf{Z\\Lambda u}\\|^2+\\|\\mathbf{u}\\|^2}{2\\sigma^2}\\right)\n\\end{aligned}\n\\]\nEvaluating the likelihood requires the marginal distribution of \\(\\mathcal{Y}\\). This can be obtained by integrating the joint distribution, \\(f_{\\mathcal{Y},\\mathcal{U}}(\\mathbf{y},\\mathbf{u})\\), evaluated at the observed \\(\\mathbf{y}\\), with respect to \\(\\mathbf{u}\\). There is an analytic solution to this integral. To derive this solution, we first write the penalized sum of squared residuals in a somewhat unusual but very useful form. Let \\(\\bbtheta\\) be the vector of parameters that determine \\(\\bblambda\\).\n\nshow(m1.θ)\n\n[0.9292213186823386, 0.018168380890783722, 0.22264487369155986]\n\n\nIn this case, \\(\\bbtheta\\) consists if the elements of the lower triangle of \\(\\bblambda\\). What we will show is that, given a value of \\(\\bbtheta\\) the maximum of the log-likelihood for that value of \\(\\bbtheta\\) and any value of \\(\\bbbeta\\) and \\(\\sigma\\) can be determined from a matrix decomposition, specifically a Cholesky decomposition shown below. This is called profiling the log likelihood.\nFor the purposes of the optimization the objective is on the deviance scale, which is negative twice the log-likelihood. A summary of the optimization can be obtained as\nm1.optsum\n\n\n\nInitialization\n\n\n\nInitial parameter vector\n[1.0, 0.0, 1.0]\n\n\nInitial objective value\n1784.642296192473\n\n\nOptimizer settings\n\n\n\nOptimizer (from NLopt)\nLN_BOBYQA\n\n\nLower bounds\n[0.0, -Inf, 0.0]\n\n\nftol_rel\n1.0e-12\n\n\nftol_abs\n1.0e-8\n\n\nxtol_rel\n0.0\n\n\nxtol_abs\n[1.0e-10, 1.0e-10, 1.0e-10]\n\n\ninitial_step\n[0.75, 1.0, 0.75]\n\n\nmaxfeval\n-1\n\n\nmaxtime\n-1.0\n\n\nResult\n\n\n\nFunction evaluations\n57\n\n\nFinal parameter vector\n[0.9292, 0.0182, 0.2226]\n\n\nFinal objective value\n1751.9393\n\n\nReturn code\nFTOL_REACHED\n\n\n\nIt required fewer than 60 evaluations of the objective function to obtain the maximum likelihood estimates of \\(\\bbtheta\\) and, with them, the estimates of all the other parameters.\nTo evaluate the log-likelihood we write the penalized sum of squared residuals in the joint density, \\(f_{\\mathcal{Y},\\mathcal{U}}(\\mathbf{y,u})\\), as\n\\[\n\\begin{aligned}\nr^2_\\bbtheta(\\mathbf{u},\\bbbeta) &=  \\|\\mathbf{y}-\\mathbf{X\\bbbeta}-\\mathbf{Z\\Lambda_\\theta u}\\|^2+\\|\\mathbf{u}\\|^2\\\\\n&=\\left\\|\\begin{bmatrix}\n\\mathbf{Z\\Lambda}&\\mathbf{X}&\\mathbf{y}\\\\\n\\mathbf{I}_q&\\mathbf{0}&\\mathbf{0}\n\\end{bmatrix}\\begin{bmatrix}-\\mathbf{u}\\\\ -\\bbbeta \\\\ 1\\end{bmatrix}\\right\\|^2 \\\\\n&= \\begin{bmatrix}-\\mathbf{u}&-\\bbbeta&1\\end{bmatrix}\n\\begin{bmatrix}\n\\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{Z\\Lambda}+\\mathbf{I} & \\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{X} & \\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{y} \\\\\n\\mathbf{X}^\\prime\\mathbf{Z\\Lambda} & \\mathbf{X}^\\prime\\mathbf{X} & \\mathbf{X}^\\prime\\mathbf{y} \\\\\n\\mathbf{y}^\\prime\\mathbf{Z\\Lambda} & \\mathbf{y}^\\prime\\mathbf{X} & \\mathbf{y}^\\prime\\mathbf{y}\n\\end{bmatrix}\n\\begin{bmatrix}-\\mathbf{u}\\\\ -\\bbbeta \\\\ 1\\end{bmatrix}\\\\\n&=\n\\begin{bmatrix}-\\mathbf{u}&-\\bbbeta&1\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{R}_{ZZ}^\\prime & \\mathbf{0} & \\mathbf{0} \\\\\n\\mathbf{R}_{ZX}^\\prime & \\mathbf{R}_{XX}^\\prime & \\mathbf{0} \\\\\n\\bbr_{Zy}^\\prime & \\bbr_{Xy}^\\prime & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{R}_{ZZ} & \\mathbf{R}_{ZX} & \\bbr_{Zy} \\\\\n\\mathbf{0} & \\mathbf{R}_{XX} & \\bbr_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}-\\mathbf{u}\\\\ -\\bbbeta \\\\ 1\\end{bmatrix}\\\\\n&= \\left\\|\n\\begin{bmatrix}\n\\mathbf{R}_{ZZ} & \\mathbf{R}_{ZX} & \\bbr_{Zy} \\\\\n\\mathbf{0} & \\mathbf{R}_{XX} & \\bbr_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\begin{bmatrix}-\\mathbf{u}\\\\ -\\bbbeta \\\\ 1\\end{bmatrix}\\right\\|^2\\\\\n&=\\|\\bbr_{Zy}-\\mathbf{R}_{ZX}\\bbbeta-\\mathbf{R}_{ZZ}\\mathbf{u}\\|^2+ \\|\\bbr_{Xy}-\\mathbf{R}_{XX}\\bbbeta\\|^2 + r_{yy}^2\\\\\n&=r_{yy}^2+\\|\\mathbf{R}_{XX}\\bbbeta-\\bbr_{Xy}\\|^2+\\|\\mathbf{R}_{ZZ}\\mathbf{u}+\\mathbf{R}_{ZX}\\bbbeta-\\bbr_{Zy}\\|^2\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mathbf{R}(\\bbtheta)=\n\\begin{bmatrix}\n\\mathbf{R}_{ZZ} & \\mathbf{R}_{ZX} & \\bbr_{Zy} \\\\\n\\mathbf{0} & \\mathbf{R}_{XX} & \\bbr_{Xy} \\\\\n\\mathbf{0} & \\mathbf{0} & r_{yy}\n\\end{bmatrix}\n\\]\nis the upper triangular, right Cholesky factor of the symmetric, positive definite matrix\n\\[\n\\begin{bmatrix}\n\\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{Z\\Lambda}+\\mathbf{I} & \\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{X} & \\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{y} \\\\\n\\mathbf{X}^\\prime\\mathbf{Z\\Lambda} & \\mathbf{X}^\\prime\\mathbf{X} & \\mathbf{X}^\\prime\\mathbf{y} \\\\\n\\mathbf{y}^\\prime\\mathbf{Z\\Lambda} & \\mathbf{y}^\\prime\\mathbf{X} & \\mathbf{y}^\\prime\\mathbf{y}\n\\end{bmatrix}\n\\]\nThe sub-matrices on the diagonal, \\(\\mathbf{R}_{ZZ}\\) and \\(\\mathbf{R}_{XX}\\), are upper triangular and \\(\\mathbf{R}_{ZZ}\\) is sparse. In our example, \\(\\mathbf{R}_{ZZ}\\) is \\(36\\times 36\\) but the only non-zeros are the upper triangles of \\(18\\) blocks of size \\(2\\times 2\\) along the diagonal. Also, the diagonal elements are, by construction, positive. Because \\(\\mathbf{R}_{ZZ}\\) is triangular its determinant, \\(|\\mathbf{R}_{ZZ}|\\), is the product of its diagonal elements which also must be positive.\nFurthermore, we can see that, for a fixed value of \\(\\bbtheta\\) the minimum \\(r^2_\\bbtheta(\\mathbf{u},\\bbbeta)\\) is \\(r_{yy}^2\\) and the conditional estimate of \\(\\bbbeta\\) satisfies\n\\[\n\\mathbf{R}_{XX}\\widehat{\\bbbeta}(\\bbtheta)=\\bbr_{Xy} .\n\\]\nThe conditional mode, \\(\\tilde{\\mathbf{u}}\\), of \\(\\mathcal{U}\\) given \\(\\mathcal{Y}=\\mathbf{y}\\) is the solution to\n\\[\n\\mathbf{R}_{ZZ}\\tilde{\\mathbf{u}}=\\bbr_{Zy}-\\mathbf{R}_{ZX}\\bbbeta\n\\]\nTechnically, \\(\\bbbeta\\) and \\(\\bbtheta\\) are assumed known because this is a statement about distributions. In practice, the estimates, \\(\\widehat{\\bbtheta}\\) and \\(\\widehat{\\beta}\\), are plugged in.\nA Cholesky decomposition can be written in terms of the lower triangular factor on the left, \\(\\bbL\\), or in terms of \\(\\mathbf{R}\\) on the right. There is a slight technical advantage in evaluating \\(\\bbL\\) in the MixedModels package so it is \\(\\bbL\\) that is evaluated and stored. However, the theory is a bit easier to see in terms of \\(\\mathbf{R}\\), which we can obtain as\n\nUpperTriangular(Matrix(sparseL(m1; full=true)'))\n\n39×39 UpperTriangular{Float64, Matrix{Float64}}:\n 3.35381  3.11966  0.0      0.0      …  3.01442   14.0118   1041.06\n  ⋅       2.3228   0.0      0.0         0.264786   8.49909   249.639\n  ⋅        ⋅       3.35381  3.11966     3.01442   14.0118    649.814\n  ⋅        ⋅        ⋅       2.3228      0.264786   8.49909    73.5188\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    699.068\n  ⋅        ⋅        ⋅        ⋅       …  0.264786   8.49909   105.851\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    915.382\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   102.27\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    935.125\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   120.416\n  ⋅        ⋅        ⋅        ⋅       …  3.01442   14.0118    930.614\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   151.279\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    957.121\n ⋮                                   ⋱                      \n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   188.483\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    927.589\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   163.961\n  ⋅        ⋅        ⋅        ⋅       …  3.01442   14.0118    887.382\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   209.185\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    893.313\n  ⋅        ⋅        ⋅        ⋅          0.264786   8.49909   145.253\n  ⋅        ⋅        ⋅        ⋅          3.01442   14.0118    963.292\n  ⋅        ⋅        ⋅        ⋅       …  0.264786   8.49909   166.733\n  ⋅        ⋅        ⋅        ⋅          3.89572    2.36568  1004.17\n  ⋅        ⋅        ⋅        ⋅           ⋅        17.0358    178.319\n  ⋅        ⋅        ⋅        ⋅           ⋅          ⋅        343.35\n\n\nTo evaluate the likelihood,\n\\[\nL(\\bbtheta,\\bbbeta,\\sigma|\\mathbf{y}) = \\int_\\mathbf{u} f_{\\mathcal{Y},\\mathcal{U}}(\\mathbf{y},\\mathbf{u})\\, d\\mathbf{u}\n\\]\nwe isolate the part of the joint density that depends on \\(\\mathbf{u}\\) and perform a change of variable to\n\\[\n\\mathbf{v}=\\mathbf{R}_{ZZ}\\mathbf{u}+\\mathbf{R}_{ZX}\\bbbeta-\\bbr_{Zy} .\n\\]\nFrom the properties of the multivariate Gaussian distribution\n\\[\n\\begin{aligned}\n\\int_{\\mathbf{u}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\\exp\\left(-\n\\frac{\\|\\mathbf{R}_{ZZ}\\mathbf{u}+\\mathbf{R}_{ZX}\\bbbeta-\\bbr_{Zy}\\|^2}{2\\sigma^2}\\right)\\,d\\mathbf{u}\n&=\\int_{\\mathbf{v}}\\frac{1}{(2\\pi\\sigma^2)^{q/2}}\\exp\\left(-\\frac{\\|\\mathbf{v}\\|^2}{2\\sigma^2}\\right)|\\mathbf{R}_{ZZ}|^{-1}\\,d\\mathbf{v}\\\\\n&=|\\mathbf{R}_{ZZ}|^{-1}\n\\end{aligned}\n\\]\nfrom which we obtain the likelihood as\n\\[\nL(\\bbtheta,\\bbbeta,\\sigma)=\\frac{|\\mathbf{R}_{ZZ}|^{-1}}{(2\\pi\\sigma^2)^{n/2}}\\exp\\left(-\n\\frac{r_{yy}^2 + \\|\\mathbf{R}_{XX}(\\bbbeta-\\widehat{\\bbbeta})\\|^2}{2\\sigma^2}\\right)\n\\]\nIf we plug in \\(\\bbbeta=\\widehat{\\bbbeta}\\) and take the logarithm we can solve for the estimate of \\(\\sigma^2\\), given \\(\\bbtheta\\)\n\\[\n\\widehat{\\sigma^2}=\\frac{r_{yy}^2}{n}\n\\]\nwhich gives the profiled log-likelihood, \\(\\ell(\\bbtheta|\\mathbf{y})=\\log L(\\bbtheta,\\widehat{\\bbbeta},\\widehat{\\sigma})\\) as\n\\[\n-2\\ell(\\bbtheta|\\mathbf{y})=2\\log(|\\mathbf{R}_{ZZ}|) +\n    n\\left(1+\\log\\left(\\frac{2\\pi r_{yy}^2(\\bbtheta)}{n}\\right)\\right)\n\\]\nThis may seem complicated but, relative to other formulations of the model, it is remarkably simple.\nOne of the interesting aspects of this formulation is that it is not necessary to solve for the conditional estimate of \\(\\bbbeta\\) or the conditional modes of the random effects when evaluating the log-likelihood. The two values needed for the log-likelihood, \\(2\\log(|\\mathbf{R}_{ZZ}|)\\) and \\(r_{yy}^2\\) are obtained directly from the Cholesky factor. The logarithm of the determinant,\n\\[\n2\\log(|\\mathbf{R}_{ZZ}|) = \\log(|\\bbLambda^\\prime\\mathbf{Z}^\\prime\\mathbf{Z}\\bbLambda+\\mathbf{I}_q|)\n\\]\nis available as\n\nlogdet(m1)\n\n73.90322078886462\n\n\nand \\(r_{yy}^2\\) is available as\n\npwrss(m1)\n\n117889.46106767072\n\n\nwhich is the square of the element in the lower right corner of either \\(\\bbL\\) or \\(\\mathbf{R}\\)\n\nabs2(last(m1.L[3]))\n\n117889.46106767072\n\n\nAlternatively, varest returns \\(\\widehat{\\sigma^2}\\)\n\nvarest(m1)\n\n654.9414503759484\n\n\nThis gives the objective function as\n\nlogdet(m1) + dof_residual(m1)*(1 + log(2π * varest(m1)))\n\n1696.004807008835\n\n\nOne last technical point, the update of the Cholesky factor, \\(\\bbL\\), for a new value of \\(\\bbtheta\\), which generates \\(\\bblambda\\) and, hence, \\(\\bbLambda\\) can start with the model matrices \\(\\mathbf{Z}\\) and \\(\\mathbf{X}\\) and the response, \\(\\mathbf{y}\\) or it can start with the products, \\(\\mathbf{Z}^\\prime\\mathbf{Z}\\), etc. The package uses the second approach which is more efficient when the number of observations is large relative to the number of random effects. The non-redundant products are stored in the A field.\nSymmetric(m1.A, :L)\nBecause the experiment is balanced, in the sense that each subject’s reaction time is measured the same number of times and after the same number of days of sleep deprivation, the diagonal blocks in \\(\\mathbf{Z}^\\prime\\mathbf{Z}\\) are repetitions of one another. The number in the lower right-hand corner of A is \\(\\mathbf{y}^\\prime\\mathbf{y}\\) or\n\nsum(abs2, sleepstudy.reaction)\n\n1.660720731910011e7"
  },
  {
    "objectID": "session3-linear-mixed-effects.html#mixed-models-and-shrinkage-of-estimates",
    "href": "session3-linear-mixed-effects.html#mixed-models-and-shrinkage-of-estimates",
    "title": "3: Linear Mixed-effects Models",
    "section": "Mixed-models and shrinkage of estimates",
    "text": "Mixed-models and shrinkage of estimates\n\nJohn Tukey characterized the regularization or shrinkage aspects of mixed-effects models as borrowing strength from the estimates for other subjects in the experiment. The estimation of the covariance matrix has the effect of shrinking an individual’s coefficients in the predictor back toward the global estimates, Figure 3.\n\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"), width=7, height=7);\nR\"\"\"\nlibrary(lme4)\nlibrary(lattice)\ndf <- coef(lme4::lmList(reaction ~ days | subj, $(DataFrame(sleepstudy))))\nfm2 <- lme4::lmer(reaction ~ days + (days|subj), $(DataFrame(sleepstudy)))\nfclow <- subset(df, `(Intercept)` < 251)\nfchigh <- subset(df, `(Intercept)` > 251)\ncc1 <- as.data.frame(coef(fm2)$subj)\nnames(cc1) <- c(\"A\", \"B\")\ndf <- cbind(df, cc1)\nff <- lme4::fixef(fm2)\nwith(df,\n     print(lattice::xyplot(`(Intercept)` ~ days, aspect = 1,\n                  x1 = B, y1 = A,\n                  panel = function(x, y, x1, y1, subscripts, ...) {\n                      panel.grid(h = -1, v = -1)\n                      x1 <- x1[subscripts]\n                      y1 <- y1[subscripts]\n                      larrows(x, y, x1, y1, type = \"closed\", length = 0.1,\n                              angle = 15, ...)\n                      lpoints(x, y,\n                              pch = trellis.par.get(\"superpose.symbol\")$pch[2],\n                              col = trellis.par.get(\"superpose.symbol\")$col[2])\n                      lpoints(x1, y1,\n                              pch = trellis.par.get(\"superpose.symbol\")$pch[1],\n                              col = trellis.par.get(\"superpose.symbol\")$col[1])\n                      lpoints(ff[2], ff[1],\n                              pch = trellis.par.get(\"superpose.symbol\")$pch[3],\n                              col = trellis.par.get(\"superpose.symbol\")$col[3])\n                      ltext(fclow[,2], fclow[,1], row.names(fclow),\n                            adj = c(0.5, 1.7))\n                      ltext(fchigh[,2], fchigh[,1], row.names(fchigh),\n                            adj = c(0.5, -0.6))\n                  },\n                  key = list(space = \"top\", columns = 3,\n                  text = list(c(\"Mixed model\", \"Within-group\", \"Population\")),\n                  points = list(col = trellis.par.get(\"superpose.symbol\")$col[1:3],\n                  pch = trellis.par.get(\"superpose.symbol\")$pch[1:3]))\n                  )))\n\"\"\";\n\n\n\n\n\nFigure 3: Shrinkage plot of the slope and intercept for each subject in the sleepstudy data (from {lattice} in R)\n\n\n\n\n\nCompare this plot to the original data plot with the lines from the various fits superimposed, Figure 4, which shows that the fits for those subjects whose data shows a strong linear trend (e.g. 308, 309, 310, 337) are not changed that much. But those whose data does not define a line well (e.g. 330, 331) are shrunk toward the global fit.\n\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"), width=7, height=5);\nR\"\"\"\nprint(xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = \"xy\",\n             layout = c(9,2), type = c(\"g\", \"p\", \"r\"),\n             coef.list = df[,3:4],\n             panel = function(..., coef.list) {\n                 panel.xyplot(...)\n                 panel.abline(as.numeric(coef.list[packet.number(),]),\n                              col.line = trellis.par.get(\"superpose.line\")$col[2],\n                              lty = trellis.par.get(\"superpose.line\")$lty[2]\n                              )\n                 panel.abline(fixef(fm2),\n                              col.line = trellis.par.get(\"superpose.line\")$col[4],\n                              lty = trellis.par.get(\"superpose.line\")$lty[4]\n                              )\n             },\n             index.cond = function(x,y) coef(lm(y ~ x))[1],\n             xlab = \"Days of sleep deprivation\",\n             ylab = \"Average reaction time (ms)\",\n             key = list(space = \"top\", columns = 3,\n             text = list(c(\"Within-subject\", \"Mixed model\", \"Population\")),\n             lines = list(col = trellis.par.get(\"superpose.line\")$col[c(2:1,4)],\n             lty = trellis.par.get(\"superpose.line\")$lty[c(2:1,4)]))))\n\"\"\";\n\n\n\n\n\nFigure 4: Average reaction time [ms.] versus days of sleep deprivation by participant with population fitted line, individual fitted lines and the mixed-model fitted lines. The panels are ordered according to increasing initial reaction time starting at the lower left."
  },
  {
    "objectID": "session3-linear-mixed-effects.html#the-sleepstudy-data",
    "href": "session3-linear-mixed-effects.html#the-sleepstudy-data",
    "title": "3a: Linear Mixed-effects Models",
    "section": "The sleepstudy data",
    "text": "The sleepstudy data\nThe sleepstudy dataset\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nis from a study on the effect of sleep deprivation on reaction time. A sample from the population of interest (long-distance truck drivers) had their average response time measured when they were on their regular sleep schedule and after one up to nine days of sleep deprivation (allowed only 3 hours per day in which to sleep).\n\n\n\n\n\n\nThis data description is inaccurate\n\n\n\n\n\nThe description of these data is inaccurate. See this description for more detail.\nUnfortunately by the time we learned this the researchers were no longer able to locate the original data. We keep using this example because it is such a nice example, even if the description is not quite accurate.\n\n\n\n\nPlot the data in a multi-panel plot, Figure 1, using the R package {lattice} via RCall.jl.\n\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"), width=7, height=5);\nR\"\"\"\nprint(\n  lattice::xyplot(\n    reaction ~ days | subj,\n    $(DataFrame(sleepstudy)),\n    type = c(\"g\",\"p\",\"r\"), layout = c(9,2),\n    index = function(x,y) coef(lm(y ~ x))[1],\n    xlab = \"Days of sleep deprivation\",\n    ylab = \"Average reaction time (ms)\",\n    aspect = \"xy\"\n  )\n)\n\"\"\";\n\n\n\n\n\nFigure 1: Average reaction time [ms.] versus days of sleep deprivation by participant. The panels are ordered according to increasing initial reaction time starting at the lower left.\n\n\n\n\n\nComments on the plot\n\nEach panel shows the data from one subject as well as a simple linear regression line fit to that subject’s data only.\nThe panels are ordered by increasing intercept of the within-subject line row-wise, starting at the bottom left.\nSome subjects, e.g. 310 and 309, have fast reaction times and are almost unaffected by the sleep deprivation.\nOthers, e.g. 337, start with slow reaction times which then increase substantially after sleep deprivation."
  },
  {
    "objectID": "session3-linear-mixed-effects.html#fitting-the-linear-mixed-effects-model",
    "href": "session3-linear-mixed-effects.html#fitting-the-linear-mixed-effects-model",
    "title": "3a: Linear Mixed-effects Models",
    "section": "Fitting the linear mixed-effects model",
    "text": "Fitting the linear mixed-effects model\n\nAs in R the model is described in a formula language, with the response to the left of the ~ character and with fixed-effects and random-effects terms to the right.\nA random-effects term is of the form (linearterms|grouping) where linearterms are terms for a linear model (which can be as simple as 1) and grouping is the name of a factor (or, less commonly, an expression), of the experimental or observational units.\n\nm1 = let\n  form = @formula reaction ~ 1 + days + (1+days|subj)\n  fit(MixedModel, form, sleepstudy)\nend\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting “contrasts”\n\n\n\n\n\nAs in R, the name contrasts is used in statistical modeling packages for Julia is the general sense of “What should be done with this categorical covariate?” It helps to indicate that the :subj covariate will be used as a grouping factor for the random effects by adding a named argument contrasts = Dict(:subj => Grouping()) in the call to fit.\nIt is not particularly important when there are 18 levels for the grouping factor, as is the case here, but when there are thousands or tens of thousands of levels it is very important to specify this contrast.\n\n\n\n\nIn a Jupyter notebook the default is to display the parameter estimates in a condensed block as above.\nMore information on the model fit can be obtained by printing the fitted model.\n\n\nprint(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n──────────────────────────────────────────────────\n\n\n\nThe fixed-effects parameters give a typical response in the population of an intercept of 251.405 ms. and a slope of 10.467 ms. per day of sleep deprivation.\nThe standard deviation of the random effects for the intercept is 23.78 ms. Thus we would expect individual intercepts to be in the range of about 200 ms. to 300 ms.\nThe standard deviation of the random effects for the slope is 5.72 ms. per day. Thus we would expect individual slopes to be in the range of about 0 ms./day to 20 ms./day.\nThe estimated correlation of the random effects for intercept and for slope is low, 0.08. We may wish to consider a model with uncorrelated random effects."
  },
  {
    "objectID": "session3-linear-mixed-effects.html#conditional-means-of-the-random-effects",
    "href": "session3-linear-mixed-effects.html#conditional-means-of-the-random-effects",
    "title": "3a: Linear Mixed-effects Models",
    "section": "“Conditional means” of the random effects",
    "text": "“Conditional means” of the random effects\n\nTechnically the random effects for each individual are not parameters per se. They are unobserved random variables. (The Bayesian formulation is a bit different but we won’t discuss that here.)\nWe can characterize the conditional distribution of the random effects given the observed data with prediction intervals, as in Figure 2.\n\n\n\nCode\ncaterpillar!(Figure(resolution=(800,450)), ranefinfo(m1,:subj))\n\n\n\n\n\nFigure 2: 95% prediction intervals on the conditional distribution of the random effects by subject, given the observed data. The subjects are ordered by increasing intercept in the conditional distributions."
  },
  {
    "objectID": "session3a-linear-mixed-effects.html",
    "href": "session3a-linear-mixed-effects.html",
    "title": "3a: Linear Mixed-effects Models",
    "section": "",
    "text": "\\[\n\\newcommand\\bbSigma{{\\boldsymbol{\\Sigma}}}\n\\]\nAttach the packages to be used"
  },
  {
    "objectID": "session3a-linear-mixed-effects.html#the-sleepstudy-data",
    "href": "session3a-linear-mixed-effects.html#the-sleepstudy-data",
    "title": "3a: Linear Mixed-effects Models",
    "section": "The sleepstudy data",
    "text": "The sleepstudy data\nThe sleepstudy dataset\n\nsleepstudy = MixedModels.dataset(:sleepstudy)\n\nArrow.Table with 180 rows, 3 columns, and schema:\n :subj      String\n :days      Int8\n :reaction  Float64\n\n\nis from a study on the effect of sleep deprivation on reaction time. A sample from the population of interest (long-distance truck drivers) had their average response time measured when they were on their regular sleep schedule and after one up to nine days of sleep deprivation (allowed only 3 hours per day in which to sleep).\n\n\n\n\n\n\nThis data description is inaccurate\n\n\n\n\n\nThe description of these data is inaccurate. See this description for more detail.\nUnfortunately by the time we learned this the researchers were no longer able to locate the original data. We keep using this example because it is such a nice example, even if the description is not quite accurate.\n\n\n\n\nPlot the data in a multi-panel plot, Figure 1, using the R package {lattice} via RCall.jl.\n\n\n\nCode\nRCall.ijulia_setdevice(MIME(\"image/svg+xml\"), width=7, height=5);\nR\"\"\"\nprint(\n  lattice::xyplot(\n    reaction ~ days | subj,\n    $(DataFrame(sleepstudy)),\n    type = c(\"g\",\"p\",\"r\"), layout = c(9,2),\n    index = function(x,y) coef(lm(y ~ x))[1],\n    xlab = \"Days of sleep deprivation\",\n    ylab = \"Average reaction time (ms)\",\n    aspect = \"xy\"\n  )\n)\n\"\"\";\n\n\n\n\n\nFigure 1: Average reaction time [ms.] versus days of sleep deprivation by participant. The panels are ordered according to increasing initial reaction time starting at the lower left.\n\n\n\n\n\nComments on the plot\n\nEach panel shows the data from one subject as well as a simple linear regression line fit to that subject’s data only.\nThe panels are ordered by increasing intercept of the within-subject line row-wise, starting at the bottom left.\nSome subjects, e.g. 310 and 309, have fast reaction times and are almost unaffected by the sleep deprivation.\nOthers, e.g. 337, start with slow reaction times which then increase substantially after sleep deprivation."
  },
  {
    "objectID": "session3a-linear-mixed-effects.html#fitting-the-linear-mixed-effects-model",
    "href": "session3a-linear-mixed-effects.html#fitting-the-linear-mixed-effects-model",
    "title": "3a: Linear Mixed-effects Models",
    "section": "Fitting the linear mixed-effects model",
    "text": "Fitting the linear mixed-effects model\n\nAs in R the model is described in a formula language, with the response to the left of the ~ character and with fixed-effects and random-effects terms to the right.\nA random-effects term is of the form (linearterms|grouping) where linearterms are terms for a linear model (which can be as simple as 1) and grouping is the name of a factor (or, less commonly, an expression), of the experimental or observational units.\n\nm1 = let\n  form = @formula reaction ~ 1 + days + (1+days|subj)\n  fit(MixedModel, form, sleepstudy)\nend\n\n\n\n\nEst.\nSE\nz\np\nσ_subj\n\n\n\n\n(Intercept)\n251.4051\n6.6323\n37.91\n<1e-99\n23.7805\n\n\ndays\n10.4673\n1.5022\n6.97\n<1e-11\n5.7168\n\n\nResidual\n25.5918\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting “contrasts”\n\n\n\n\n\nAs in R, the name contrasts is used in statistical modeling packages for Julia is the general sense of “What should be done with this categorical covariate?” It helps to indicate that the :subj covariate will be used as a grouping factor for the random effects by adding a named argument contrasts = Dict(:subj => Grouping()) in the call to fit.\nIt is not particularly important when there are 18 levels for the grouping factor, as is the case here, but when there are thousands or tens of thousands of levels it is very important to specify this contrast.\n\n\n\n\nIn a Jupyter notebook the default is to display the parameter estimates in a condensed block as above.\nMore information on the model fit can be obtained by printing the fitted model.\n\n\nprint(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n──────────────────────────────────────────────────\n\n\n\nThe fixed-effects parameters give a typical response in the population of an intercept of 251.405 ms. and a slope of 10.467 ms. per day of sleep deprivation.\nThe standard deviation of the random effects for the intercept is 23.78 ms. Thus we would expect individual intercepts to be in the range of about 200 ms. to 300 ms.\nThe standard deviation of the random effects for the slope is 5.72 ms. per day. Thus we would expect individual slopes to be in the range of about 0 ms./day to 20 ms./day.\nThe estimated correlation of the random effects for intercept and for slope is low, 0.08. We may wish to consider a model with uncorrelated random effects."
  },
  {
    "objectID": "session3a-linear-mixed-effects.html#conditional-means-of-the-random-effects",
    "href": "session3a-linear-mixed-effects.html#conditional-means-of-the-random-effects",
    "title": "3a: Linear Mixed-effects Models",
    "section": "“Conditional means” of the random effects",
    "text": "“Conditional means” of the random effects\n\nTechnically the random effects for each individual are not parameters per se. They are unobserved random variables. (The Bayesian formulation is a bit different but we won’t discuss that here.)\nWe can characterize the conditional distribution of the random effects given the observed data with prediction intervals, as in Figure 2.\n\n\n\nCode\ncaterpillar!(Figure(resolution=(800,450)), ranefinfo(m1,:subj))\n\n\n\n\n\nFigure 2: 95% prediction intervals on the conditional distribution of the random effects by subject, given the observed data. The subjects are ordered by increasing intercept in the conditional distributions."
  },
  {
    "objectID": "session3b-glmm.html",
    "href": "session3b-glmm.html",
    "title": "3b: Generalized linear mixed models",
    "section": "",
    "text": "Load the packages to be used"
  },
  {
    "objectID": "session3b-glmm.html#matrix-notation-for-the-sleepstudy-model",
    "href": "session3b-glmm.html#matrix-notation-for-the-sleepstudy-model",
    "title": "3b: Generalized linear mixed models",
    "section": "Matrix notation for the sleepstudy model",
    "text": "Matrix notation for the sleepstudy model\n\nsleepstudy = DataFrame(MixedModels.dataset(:sleepstudy))\n\n\n180 rows × 3 columnssubjdaysreactionStringInt8Float641S3080249.562S3081258.7053S3082250.8014S3083321.445S3084356.8526S3085414.697S3086382.2048S3087290.1499S3088430.58510S3089466.35311S3090222.73412S3091205.26613S3092202.97814S3093204.70715S3094207.71616S3095215.96217S3096213.6318S3097217.72719S3098224.29620S3099237.31421S3100199.05422S3101194.33223S3102234.3224S3103232.84225S3104229.30726S3105220.45827S3106235.42128S3107255.75129S3108261.01230S3109247.515⋮⋮⋮⋮\n\n\n\ncontrasts = Dict(:subj => Grouping())\nm1 = let\n  form = @formula(reaction ~ 1 + days + (1 + days | subj))\n  fit(MixedModel, form, sleepstudy; contrasts)\nend\nprintln(m1)\n\nLinear mixed model fit by maximum likelihood\n reaction ~ 1 + days + (1 + days | subj)\n   logLik   -2 logLik     AIC       AICc        BIC    \n  -875.9697  1751.9393  1763.9393  1764.4249  1783.0971\n\nVariance components:\n            Column    Variance Std.Dev.   Corr.\nsubj     (Intercept)  565.51067 23.78047\n         days          32.68212  5.71683 +0.08\nResidual              654.94145 25.59182\n Number of obs: 180; levels of grouping factors: 18\n\n  Fixed-effects parameters:\n──────────────────────────────────────────────────\n                Coef.  Std. Error      z  Pr(>|z|)\n──────────────────────────────────────────────────\n(Intercept)  251.405      6.63226  37.91    <1e-99\ndays          10.4673     1.50224   6.97    <1e-11\n──────────────────────────────────────────────────\n\n\nThe response vector, y, has 180 elements. The fixed-effects coefficient vector, β, has 2 elements and the fixed-effects model matrix, X, is of size 180 × 2.\n\nm1.y\n\n180-element view(::Matrix{Float64}, :, 3) with eltype Float64:\n 249.56\n 258.7047\n 250.8006\n 321.4398\n 356.8519\n 414.6901\n 382.2038\n 290.1486\n 430.5853\n 466.3535\n 222.7339\n 205.2658\n 202.9778\n   ⋮\n 350.7807\n 369.4692\n 269.4117\n 273.474\n 297.5968\n 310.6316\n 287.1726\n 329.6076\n 334.4818\n 343.2199\n 369.1417\n 364.1236\n\n\n\nm1.β\n\n2-element Vector{Float64}:\n 251.40510484848375\n  10.467285959595792\n\n\n\nm1.X\n\n180×2 Matrix{Float64}:\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n ⋮    \n 1.0  8.0\n 1.0  9.0\n 1.0  0.0\n 1.0  1.0\n 1.0  2.0\n 1.0  3.0\n 1.0  4.0\n 1.0  5.0\n 1.0  6.0\n 1.0  7.0\n 1.0  8.0\n 1.0  9.0\n\n\nThe second column of X is just the days vector and the first column is all 1’s.\nThere are 36 random effects, 2 for each of the 18 levels of subj. The “estimates” (technically, the conditional means or conditional modes) are returned as a vector of matrices, one matrix for each grouping factor. In this case there is only one grouping factor for the random effects so there is one one matrix which contains 18 intercept random effects and 18 slope random effects.\n\nm1.b\n\n1-element Vector{Matrix{Float64}}:\n [2.8158191325913045 -40.04844170604461 … 0.7232620377478233 12.118907799956531; 9.07551171690184 -8.644079452057246 … -0.9710526342424729 1.31069806890679]\n\n\n\nonly(m1.b)   # only one grouping factor\n\n2×18 Matrix{Float64}:\n 2.81582  -40.0484   -38.4331  22.8321   …  -24.7101   0.723262  12.1189\n 9.07551   -8.64408   -5.5134  -4.65872       4.6597  -0.971053   1.3107\n\n\nThere is a model matrix, \\(\\mathbf{Z}\\), for the random effects. In general it has one chunk of columns for the first grouping factor, a chunk of columns for the second grouping factor, etc.\nIn this case there is only one grouping factor.\n\nInt.(only(m1.reterms))\n\n180×36 Matrix{Int64}:\n 1  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  1  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  2  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  3  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  4  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  5  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 1  6  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  7  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  8  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 1  9  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  1  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n 0  0  1  2  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0\n ⋮              ⋮              ⋮        ⋱     ⋮              ⋮              ⋮\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  8  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  1  9  0  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  0\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  1\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  2\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  3\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  4\n 0  0  0  0  0  0  0  0  0  0  0  0  0  …  0  0  0  0  0  0  0  0  0  0  1  5\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  6\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  7\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  8\n 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  1  9\n\n\nThe defining property of a linear model or linear mixed model is that the fitted values are linear combinations of the fixed-effects parameters and the random effects. We can write the fitted values as\n\nm1.X * m1.β + only(m1.reterms) * vec(only(m1.b))\n\n180-element Vector{Float64}:\n 254.22092398107506\n 273.7637216575727\n 293.3065193340703\n 312.849317010568\n 332.3921146870656\n 351.93491236356317\n 371.47771004006086\n 391.0205077165585\n 410.5633053930561\n 430.1061030695537\n 211.35666314243912\n 213.1798696499777\n 215.0030761575162\n   ⋮\n 328.09823348905815\n 337.5944668144114\n 263.52401264844025\n 275.3019966769429\n 287.07998070544545\n 298.85796473394805\n 310.63594876245065\n 322.4139327909532\n 334.1919168194558\n 345.9699008479584\n 357.74788487646094\n 369.5258689049635\n\n\n\nfitted(m1)   # just to check that these are indeed the same as calculated above\n\n180-element Vector{Float64}:\n 254.22092398107506\n 273.76372165757266\n 293.3065193340703\n 312.8493170105679\n 332.3921146870656\n 351.93491236356317\n 371.47771004006086\n 391.0205077165585\n 410.5633053930561\n 430.1061030695537\n 211.35666314243912\n 213.1798696499777\n 215.0030761575162\n   ⋮\n 328.0982334890581\n 337.5944668144114\n 263.52401264844025\n 275.3019966769429\n 287.07998070544545\n 298.857964733948\n 310.63594876245065\n 322.4139327909532\n 334.19191681945574\n 345.9699008479584\n 357.74788487646094\n 369.5258689049635\n\n\nIn symbols we would write the linear predictor expression as\n\\[\n\\boldsymbol{\\eta} = \\mathbf{X}\\boldsymbol{\\beta} +\\mathbf{Z b}\n\\]\nwhere \\(\\boldsymbol{\\eta}\\) has 180 elements, \\(\\boldsymbol{\\beta}\\) has 2 elements, \\(\\bf b\\) has 36 elements, \\(\\bf X\\) is of size 180 × 2 and \\(\\bf Z\\) is of size 180 × 36.\nFor a linear model or linear mixed model the linear predictor is the mean response, \\(\\boldsymbol\\mu\\). That is, we can write the probability model in terms of a 180-dimensional random variable, \\(\\mathcal Y\\), for the response and a 36-dimensional random variable, \\(\\mathcal B\\), for the random effects as\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\boldsymbol{\\Sigma}_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{\\Sigma}_\\boldsymbol{\\theta}\\) is a 36 × 36 symmetric covariance matrix that has a special form - it consists of 18 diagonal blocks, each of size 2 × 2 and all the same.\nThis symmetric matrix can be constructed from the parameters \\(\\boldsymbol\\theta\\), which generate the lower triangular matrix \\(\\boldsymbol\\lambda\\), and the estimate \\(\\widehat{\\sigma^2}\\).\n\nm1.θ\n\n3-element Vector{Float64}:\n 0.9292213186823386\n 0.018168380890783722\n 0.22264487369155986\n\n\n\nλ = only(m1.λ)  # with multiple grouping factors there will be multiple λ's\n\n2×2 LinearAlgebra.LowerTriangular{Float64, Matrix{Float64}}:\n 0.929221    ⋅ \n 0.0181684  0.222645\n\n\n\nΣ = varest(m1) * (λ * λ')\n\n2×2 Matrix{Float64}:\n 565.511  11.057\n  11.057  32.6821\n\n\nCompare the diagonal elements to the Variance column of\nVarCorr(m1)\n\n\n\n\nColumn\nVariance\nStd.Dev\nCorr.\n\n\n\n\nsubj\n(Intercept)\n565.51067\n23.78047\n\n\n\n\ndays\n32.68212\n5.71683\n+0.08\n\n\nResidual\n\n654.94145\n25.59182"
  },
  {
    "objectID": "session3b-glmm.html#linear-predictors-in-lmms-and-glmms",
    "href": "session3b-glmm.html#linear-predictors-in-lmms-and-glmms",
    "title": "3b: Generalized linear mixed models",
    "section": "Linear predictors in LMMs and GLMMs",
    "text": "Linear predictors in LMMs and GLMMs\nWriting the model for \\(\\mathcal Y\\) as\n\\[\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b})\\sim\\mathcal{N}(\\bf{ X\\boldsymbol\\beta + Z b},\\sigma^2\\bf{I})\n\\]\nmay seem like over-mathematization (or “overkill”, if you prefer) relative to expressions like\n\\[\ny_i = \\beta_1 x_{i,1} + \\beta_2 x_{i,2}+ b_1 z_{i,1} +\\dots+b_{36} z_{i,36}+\\epsilon_i\n\\]\nbut this more abstract form is necessary for generalizations.\nThe way that I read the first form is\n\n\n\n\n\n\nThe conditional distribution of the response vector, \\(\\mathcal Y\\), given that the random effects vector, \\(\\mathcal B =\\bf b\\), is a multivariate normal (or Gaussian) distribution whose mean, \\(\\boldsymbol\\mu\\), is the linear predictor, \\(\\boldsymbol\\eta=\\bf{X\\boldsymbol\\beta+Zb}\\), and whose covariance matrix is \\(\\sigma^2\\bf I\\). That is, conditional on \\(\\bf b\\), the elements of \\(\\mathcal Y\\) are independent normal random variables with constant variance, \\(\\sigma^2\\), and means of the form \\(\\boldsymbol\\mu = \\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\).\n\n\n\nSo the only things that differ in the distributions of the \\(y_i\\)’s are the means and they are determined by this linear predictor, \\(\\boldsymbol\\eta = \\bf{X\\boldsymbol\\beta+Zb}\\)."
  },
  {
    "objectID": "session3b-glmm.html#generalized-linear-mixed-models",
    "href": "session3b-glmm.html#generalized-linear-mixed-models",
    "title": "3b: Generalized linear mixed models",
    "section": "Generalized Linear Mixed Models",
    "text": "Generalized Linear Mixed Models\nConsider first a GLMM for a vector, \\(\\bf y\\), of binary (i.e. yes/no) responses. The probability model for the conditional distribution \\(\\mathcal Y|\\mathcal B=\\bf b\\) consists of independent Bernoulli distributions where the mean, \\(\\mu_i\\), for the i’th response is again determined by the i’th element of a linear predictor, \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol\\beta+\\mathbf{Z b}\\).\nHowever, in this case we will run into trouble if we try to make \\(\\boldsymbol\\mu=\\boldsymbol\\eta\\) because \\(\\mu_i\\) is the probability of “success” for the i’th response and must be between 0 and 1. We can’t guarantee that the i’th component of \\(\\boldsymbol\\eta\\) will be between 0 and 1. To get around this problem we apply a transformation to take \\(\\eta_i\\) to \\(\\mu_i\\). For historical reasons this transformation is called the inverse link, written \\(g^{-1}\\), and the opposite transformation - from the probability scale to an unbounded scale - is called the link, g.\nEach probability distribution in the exponential family (which is most of the important ones), has a canonical link which comes from the form of the distribution itself. The details aren’t as important as recognizing that the distribution itself determines a preferred link function.\nFor the Bernoulli distribution, the canonical link is the logit or log-odds function,\n\\[\n\\eta = g(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right),\n\\]\n(it’s called log-odds because it is the logarithm of the odds ratio, \\(p/(1-p)\\)) and the canonical inverse link is the logistic\n\\[\n\\mu=g^{-1}(\\eta)=\\frac{1}{1+\\exp(-\\eta)}.\n\\]\nThis is why fitting a binary response is sometimes called logistic regression.\nFor later use we define a Julia logistic function. See this presentation for more information than you could possible want to know on how Julia converts code like this to run on the processor.\n\nincrement(x) = x + one(x)\nlogistic(η) = inv(increment(exp(-η)))\n\nlogistic (generic function with 1 method)\n\n\nTo reiterate, the probability model for a Generalized Linear Mixed Model (GLMM) is\n\\[\n\\begin{aligned}\n(\\mathcal{Y} | \\mathcal{B}=\\bf{b}) &\\sim\\mathcal{D}(\\bf{g^{-1}(X\\boldsymbol\\beta + Z b)},\\phi)\\\\\\\\\n\\mathcal{B}&\\sim\\mathcal{N}(\\bf{0},\\Sigma_{\\boldsymbol\\theta}) .\n\\end{aligned}\n\\]\nwhere \\(\\mathcal{D}\\) is the distribution family (such as Bernoulli or Poisson), \\(g^{-1}\\) is the inverse link and \\(\\phi\\) is a scale parameter for \\(\\mathcal{D}\\) if it has one. The important cases of the Bernoulli and Poisson distributions don’t have a scale parameter - once you know the mean you know everything you need to know about the distribution. (For those following the presentation, this poem by John Keats is the one with the couplet “Beauty is truth, truth beauty - that is all ye know on earth and all ye need to know.”)\n\nAn example of a Bernoulli GLMM\n\nThe contra dataset in the MixedModels package is from a survey on the use of artificial contraception by women in Bangladesh.\n\n\ncontra = DataFrame(MixedModels.dataset(:contra))\n\n\n1,934 rows × 5 columnsdisturbanlivchageuseStringStringStringFloat64String1D01Y3+18.44N2D01Y0-5.56N3D01Y21.44N4D01Y3+8.44N5D01Y0-13.56N6D01Y0-11.56N7D01Y3+18.44N8D01Y3+-3.56N9D01Y1-5.56N10D01Y3+1.44N11D01Y0-11.56Y12D01Y0-2.56N13D01Y1-4.56N14D01Y3+5.44N15D01Y3+-0.56N16D01Y3+4.44Y17D01Y0-5.56N18D01Y3+-0.56Y19D01Y1-6.56Y20D01Y2-3.56N21D01Y0-4.56N22D01Y0-9.56N23D01Y3+2.44N24D01Y22.44Y25D01Y1-4.56Y26D01Y3+14.44N27D01Y0-6.56Y28D01Y1-3.56Y29D01Y1-5.56Y30D01Y1-1.56Y⋮⋮⋮⋮⋮⋮\n\n\n\ncombine(groupby(contra, :dist), nrow)\n\n\n60 rows × 2 columnsdistnrowStringInt641D011172D02203D0324D04305D05396D06657D07188D08379D092310D101311D112112D122913D132414D1411815D152216D162017D172418D184719D192620D201521D211822D222023D231524D241425D256726D261327D274428D284929D293230D3061⋮⋮⋮\n\n\n\nThe information recorded included woman’s age, the number of live children she has, whether she lives in an urban or rural setting, and the political district in which she lives.\nThe age was centered. Unfortunately, the version of the data to which I had access did not record what the centering value was.\nA data plot, Figure 1, shows that the probability of contraception use is not linear in age - it is low for younger women, higher for women in the middle of the range (assumed to be women in late 20’s to early 30’s) and low again for older women (late 30’s to early 40’s in this survey).\nIf we fit a model with only the age term in the fixed effects, that term will not be significant. This doesn’t mean that there is no “age effect”, it only means that there is no significant linear effect for age.\n\n\n\nCode\ndraw(\n  data(\n    @transform(\n      contra,\n      :numuse = Int(:use == \"Y\"),\n      :urb = ifelse(:urban == \"Y\", \"Urban\", \"Rural\")\n    )\n  ) *\n  mapping(\n    :age => \"Centered age (yr)\",\n    :numuse => \"Frequency of contraception use\";\n    col=:urb,\n    color=:livch,\n  ) *\n  smooth();\n  figure=(; resolution=(800, 450)),\n)\n\n\n\n\n\nFigure 1: Smoothed relative frequency of contraception use versus centered age for women in the 1989 Bangladesh Fertility Survey\n\n\n\n\n\ncontrasts = Dict(\n  :dist => Grouping(),\n  :urban => HelmertCoding(),\n  :children => HelmertCoding(),\n)\nnAGQ = 9\ndist = Bernoulli()\ngm1 = let\n  form = @formula(\n    use ~ 1 + age + abs2(age) + urban + livch + (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 217   Time: 0:00:00 ( 0.90 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.6871\n0.1686\n-4.08\n<1e-04\n0.4786\n\n\nage\n0.0035\n0.0092\n0.38\n0.7022\n\n\n\nabs2(age)\n-0.0046\n0.0007\n-6.29\n<1e-09\n\n\n\nurban: Y\n0.3484\n0.0600\n5.81\n<1e-08\n\n\n\nlivch: 1\n0.8152\n0.1622\n5.02\n<1e-06\n\n\n\nlivch: 2\n0.9165\n0.1851\n4.95\n<1e-06\n\n\n\nlivch: 3+\n0.9154\n0.1858\n4.93\n<1e-06\n\n\n\n\n\n\n\nNotice that the linear term for age is not significant but the quadratic term for age is highly significant.\nWe usually retain the lower order term, even if it is not significant, if the higher order term is significant.\nNotice also that the parameter estimates for the treatment contrasts for livch are similar. Thus the distinction of 1, 2, or 3+ childen is not as important as the contrast between having any children and not having any. Those women who already have children are more likely to use artificial contraception.\nFurthermore, the women without children have a different probability vs age profile than the women with children. To allow for this we define a binary children factor and incorporate an age&children interaction.\n\nVarCorr(gm1)\n\n\n\n\nColumn\nVariance\nStd.Dev\n\n\n\n\ndist\n(Intercept)\n0.229090\n0.478634\n\n\n\n\nNotice that there is no “residual” variance being estimated. This is because the Bernoulli distribution doesn’t have a scale parameter.\n\n\n\nConvert livch to a binary factor\n\n@transform!(contra, :children = :livch ≠ \"0\")\n\n\n1,934 rows × 6 columnsdisturbanlivchageusechildrenStringStringStringFloat64StringBool1D01Y3+18.44N12D01Y0-5.56N03D01Y21.44N14D01Y3+8.44N15D01Y0-13.56N06D01Y0-11.56N07D01Y3+18.44N18D01Y3+-3.56N19D01Y1-5.56N110D01Y3+1.44N111D01Y0-11.56Y012D01Y0-2.56N013D01Y1-4.56N114D01Y3+5.44N115D01Y3+-0.56N116D01Y3+4.44Y117D01Y0-5.56N018D01Y3+-0.56Y119D01Y1-6.56Y120D01Y2-3.56N121D01Y0-4.56N022D01Y0-9.56N023D01Y3+2.44N124D01Y22.44Y125D01Y1-4.56Y126D01Y3+14.44N127D01Y0-6.56Y028D01Y1-3.56Y129D01Y1-5.56Y130D01Y1-1.56Y1⋮⋮⋮⋮⋮⋮⋮\n\n\n\ngm2 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 133   Time: 0:00:00 ( 0.91 ms/it)\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\n\n\n\nVarious model-fit statistics\n\n\n\nCode\nlet\n  mods = [gm2, gm1]\n  DataFrame(;\n    model=[:gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n2 rows × 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm272364.922379.182418.152379.242gm182372.462388.732433.272388.81\n\n\n\nBecause these models are not nested, we cannot do a likelihood ratio test. Nevertheless we see that the deviance is much lower in the model with age & children even though the 3 levels of livch have been collapsed into a single level of children.\nThere is a substantial decrease in the deviance even though there are fewer parameters in model gm2 than in gm1. This decrease is because the flexibility of the model - its ability to model the behavior of the response - is being put to better use in gm2 than in gm1.\n\n\n\nUsing urban&dist as a grouping factor\n\nIt turns out that there can be more difference between urban and rural settings within the same political district than there is between districts. To model this difference we build a model with urban&dist as a grouping factor.\n\n\ngm3 = let\n  form = @formula(\n    use ~\n      1 +\n      age * children +\n      abs2(age) +\n      children +\n      urban +\n      (1 | urban & dist)\n  )\n  fit(MixedModel, form, contra, dist; nAGQ, contrasts)\nend\n\nMinimizing 158   Time: 0:00:00 ( 0.92 ms/it)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\n\n\n\nCode\nlet\n  mods = [gm3, gm2, gm1]\n  DataFrame(;\n    model=[:gm3, :gm2, :gm1],\n    npar=dof.(mods),\n    deviance=deviance.(mods),\n    AIC=aic.(mods),\n    BIC=bic.(mods),\n    AICc=aicc.(mods),\n  )\nend\n\n\n\n3 rows × 6 columnsmodelnpardevianceAICBICAICcSymbolInt64Float64Float64Float64Float641gm372353.822368.482407.462368.542gm272364.922379.182418.152379.243gm182372.462388.732433.272388.81\n\n\n\nNotice that the parameter count in gm3 is the same as that of gm2 - the thing that has changed is the number of levels of the grouping factor- resulting in a much lower deviance for gm3. A simple count of the number of parameters to be estimated does not always reflect the complexity of the model.\n\ngm2\n\n\n\n\nEst.\nSE\nz\np\nσ_dist\n\n\n\n\n(Intercept)\n-0.3614\n0.1275\n-2.83\n0.0046\n0.4756\n\n\nage\n-0.0131\n0.0110\n-1.19\n0.2352\n\n\n\nchildren: true\n0.6054\n0.1035\n5.85\n<1e-08\n\n\n\nabs2(age)\n-0.0058\n0.0008\n-6.89\n<1e-11\n\n\n\nurban: Y\n0.3567\n0.0602\n5.93\n<1e-08\n\n\n\nage & children: true\n0.0342\n0.0127\n2.69\n0.0072\n\n\n\n\ngm3\n\n\n\n\n\n\n\n\n\n\n\n\nEst.\nSE\nz\np\nσ_urban & dist\n\n\n\n\n(Intercept)\n-0.3415\n0.1269\n-2.69\n0.0071\n0.5761\n\n\nage\n-0.0129\n0.0112\n-1.16\n0.2471\n\n\n\nchildren: true\n0.6064\n0.1045\n5.80\n<1e-08\n\n\n\nabs2(age)\n-0.0056\n0.0008\n-6.66\n<1e-10\n\n\n\nurban: Y\n0.3936\n0.0859\n4.58\n<1e-05\n\n\n\nage & children: true\n0.0332\n0.0128\n2.59\n0.0096\n\n\n\n\n\nThe coefficient for age may be regarded as insignificant but we retain it for two reasons: we have a term of age² (written abs2(age)) in the model and we have a significant interaction age & children in the model."
  },
  {
    "objectID": "session3b-glmm.html#summarizing-the-results",
    "href": "session3b-glmm.html#summarizing-the-results",
    "title": "3b: Generalized linear mixed models",
    "section": "Summarizing the results",
    "text": "Summarizing the results\n\nFrom the data plot we can see a quadratic trend in the probability by age.\nThe patterns for women with children are similar and we do not need to distinguish between 1, 2, and 3+ children.\nWe do distinguish between those women who do not have children and those with children. This shows up in a signficant age & children interaction term."
  }
]