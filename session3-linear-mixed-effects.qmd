---
title: "3: Linear Mixed-effects Models"
author: "Douglas Bates and Claudia Solis-Lemus"
jupyter: julia-1.8
---

::: {.hidden}
$$
\newcommand\bbA{{\mathbf{A}}}
\newcommand\bbI{{\mathbf{I}}}
\newcommand\bbL{{\mathbf{L}}}
\newcommand\bbX{{\mathbf{X}}}
\newcommand\bbb{{\mathbf{b}}}
\newcommand\bbr{{\mathbf{r}}}
\newcommand\bbx{{\mathbf{x}}}
\newcommand\bby{{\mathbf{y}}}
\newcommand\bbbeta{{\boldsymbol{\beta}}}
\newcommand\bbeta{{\boldsymbol{\eta}}}
\newcommand\bbLambda{{\boldsymbol{\Lambda}}}
\newcommand\bblambda{{\boldsymbol{\lambda}}}
\newcommand\bbOmega{{\boldsymbol{\Omega}}}
\newcommand\bbmu{{\boldsymbol{\mu}}}
\newcommand\bbSigma{{\boldsymbol{\Sigma}}}
\newcommand\bbtheta{{\boldsymbol{\theta}}}
\newcommand\mcN{{\mathcal{N}}}
\newcommand\mcB{{\mathcal{B}}}
\newcommand\mcU{{\mathcal{U}}}
\newcommand\mcX{{\mathcal{X}}}
\newcommand\mcY{{\mathcal{Y}}}
\newcommand\mcZ{{\mathcal{Z}}}
$$
:::

Attach the packages to be used

```{julia}
#| code-fold: true
using CairoMakie        # graphics backend
using DataFrames
using LinearAlgebra
using MixedModels
using MixedModelsMakie  # special graphics for mixed models
using ProgressMeter     # report iteration speed when fitting models
using RCall

CairoMakie.activate!(; type="svg")     # Scalable Vector Graphics backend
ProgressMeter.ijulia_behavior(:clear); # Adjust progress meter for Jupyter output
```


# Linear Mixed Models in Julia

- A _mixed-effects model_ or, more simply, a _mixed model_ incorporates both _fixed-effects_ parameters and _random effects_.
- The random effects are associated with the levels of one or more _grouping factors_, which typically are _experimental units_ or _observational units_, such as `subject` or `item`.
- From an experimental design point of view these are _blocking variables_: known sources of variability for which we wish to account but whose levels are not themselves of interest.
- This is in contrast to _experimental_ or _observational_ factors with a known, fixed set of levels that we are seeking to compare.
- [MixedModels.jl](https://github.com/JuliaStats/MixedModels.jl) provides structures and methods for fitting and analyzing mixed-effects models.
- Fitting Linear Mixed Models (LMMs) or Generalized Linear Mixed Models (GLMMs) is described in these [notes](https://repsychling.github.io/SMLP2022) and in this [in-progress book](https://juliamixedmodels.github.io/EmbraceUncertainty).

## The sleepstudy data

The [`sleepstudy`] dataset

```{julia}
sleepstudy = MixedModels.dataset(:sleepstudy)
```

is from a study on the effect of sleep deprivation on reaction time.
A sample from the population of interest (long-distance truck drivers) had their average response time measured when they were on their regular sleep schedule and after one up to nine days of sleep deprivation (allowed only 3 hours per day in which to sleep).

::: {.callout-warning collapse="true"}
### This data description is inaccurate

The description of these data is inaccurate.
See [this description](https://repsychling.github.io/SMLP2022/sleepstudy_speed.html) for more detail.

Unfortunately by the time we learned this the researchers were no longer able to locate the original data.
We keep using this example because it is such a nice example, even if the description is not quite accurate.
:::

- Plot the data in a multi-panel plot, @fig-sleepxy, using the R package `{lattice}` via [RCall.jl](https://github.com/JuliaInterop/RCall.jl).

```{julia}
#| code-fold: true
#| fig-cap: "Average reaction time [ms.] versus days of sleep deprivation by participant.  The panels are ordered according to increasing initial reaction time starting at the lower left."
#| label: fig-sleepxy
RCall.ijulia_setdevice(MIME("image/svg+xml"), width=7, height=5);
R"""
print(
  lattice::xyplot(
    reaction ~ days | subj,
    $(DataFrame(sleepstudy)),
    type = c("g","p","r"), layout = c(9,2),
    index = function(x,y) coef(lm(y ~ x))[1],
    xlab = "Days of sleep deprivation",
    ylab = "Average reaction time (ms)",
    aspect = "xy"
  )
)
""";
```

### Comments on the plot

- Each panel shows the data from one subject as well as a simple linear regression line fit to that subject's data only.

- The panels are ordered by increasing intercept of the within-subject line row-wise, starting at the bottom left.

- Some subjects, e.g. 310 and 309, have fast reaction times and are almost unaffected by the sleep deprivation.  

- Others, e.g. 337, start with slow reaction times which then increase substantially after sleep deprivation.

### Formulating a model

- A suitable model for these data would include an intercept and slope for the "typical" subject and randomly distributed deviations from these values for each of the observed subjects.

- The intercept and slope of the "typical" response over the population are parameters to be estimated (i.e. fixed effect parameters).

- The intercept and slope deviations from the population values for each subject are random variables (i.e. random effects).

- The assumed distribution of the random effects vector is multivariate Gaussian with mean zero (because they represent deviations from the population parameters) and an unknown covariance matrix, $\bbSigma$, to be estimated from the data.

- Because $\bbSigma$ is a covariance matrix it must be symmetric and be positive-definite, a condition that is similar to the requirement that a scalar variance must be positive.

## Fitting the linear mixed-effects model

- As in R the model is described in a formula language, with the response to the left of the `~` character and with fixed-effects and random-effects terms to the right.

- A random-effects term is of the form `(linearterms|grouping)` where `linearterms` are terms for a linear model (which can be as simple as `1`) and `grouping` is the name of a factor (or, less commonly an expression), of the experimental or observational units.

```{julia}
m1 = let
  form = @formula reaction ~ 1 + days + (1+days|subj)
  fit(MixedModel, form, sleepstudy)
end
```

::: {.callout-note collapse="true"}
### Setting "contrasts"

As in R, the name `contrasts` is used in statistical modeling packages for Julia is the general sense of "What should be done with this categorical covariate?"
It helps to indicate that the `:subj` covariate will be used as a grouping factor for the random effects by adding a named argument `contrasts = Dict(:subj => Grouping())` in the call to `fit`.

It is not particularly important when there are 18 levels for the grouping factor, as is the case here, but when there are thousands or tens of thousands of levels it is very important to specify this contrast.
:::

- In a Jupyter notebook the default is to display the parameter estimates in a condensed block as above.

- More information on the model fit can be obtained by `print`ing the fitted model.

```{julia}
print(m1)
```

- The fixed-effects parameters give a typical response in the population of an intercept of 251.405 ms. and a slope of 10.467 ms. per day of sleep deprivation.

- The standard deviation of the random effects for the intercept is 23.78 ms.  Thus we would expect individual intercepts to be in the range of about 200 ms. to 300 ms.

- The standard deviation of the random effects for the slope is 5.72 ms. per day.  Thus we would expect individual slopes to be in the range of about 0 ms./day to 20 ms./day.

- The correlation of the random effects for intercept and for slope is low, -0.08.  We may wish to consider a model with uncorrelated random effects.

## "Conditional means" of the random effects

- Technically the random effects for each individual are not parameters per se.  They are unobserved random variables.  (The Bayesian formulation is a bit different but we won't discuss that here.)

- We can characterize the conditional distribution of the random effects given the observed data with prediction intervals, as in @fig-sleepcaterpillar.

```{julia}
#| code-fold: true
#| fig-cap: "95% prediction intervals on the conditional distribution of the random effects by subject, given the observed data. The subjects are ordered by increasing intercept in the conditional distributions.
#| label: fig-sleepcaterpillar
caterpillar!(Figure(resolution=(800,450)), ranefinfo(m1,:subj))
```


Thus the (maximum likelihood) estimate of the covariance matrix $\bbSigma$ is

```{julia}
Σ = σ² * λ * λ'
```

The correlation shown in the "Variance components" table can be evaluated as

```{julia}
Σ[2,1] / sqrt(Σ[1,1] * Σ[2,2])
```

## The Big Picture

Although it is tempting to construct the model on a per-subject basis it is ultimately easier to consider the entire set of responses and the collection of all of the random effects together.
There are two reasons for this.
First, the parameters must be estimated from the complete data set.
Second, in situations where there is more than one grouping factor for the random effects it may not be possible to partition the responses according to the grouping factor.
In the sleepstudy example the 180 observations can be partitioned into eighteen groups of ten observations on each of the eighteen subjects.
However, in an example we will consider below each observation is on one of 56 subjects and one of 32 items and those classifications are _crossed_.
That is, each subject is tested on each item and each item is tested on each subject.
(Well, that was the plan at least.
As often happens a few observations were erroneously recorded so the factors are not completely crossed in the data after cleaning.)

In any case, we write $\bbb$ for the complete random-effects vector (in this case a 36-dimensional vector formed from the $2\times 18$ matrix in _column-major_ order).

```{julia}
b = vec(first(ranef(m1)))
```

In the model the unconditional distribution of the random variable $\mathcal{B}$ is
$$
\mathcal{B}\sim\mathcal{N}\left(\mathbf{0},\sigma^2\bbLambda\bbLambda^\prime\right)
$$
and the conditional distribution of the response vector, $\mathcal{Y}$, is
$$
(\mathcal{Y}|\mathcal{B}=\bbb)\sim\mathcal{N}\left(\mathbf{X\bbbeta}+\mathbf{Zb}, \sigma^2\mathbf{I}_n\right)
$$

The model matrix $\mathbf{X}$ for the fixed-effects has the usual form

```{julia}
Int.(m1.X)  # display as Int to reduce clutter
```

but the model matrix $\mathbf{Z}$ for the random effects is very sparse.  That is, most of the entries in $\mathbf{Z}$ are zero.

```{julia}
Int.(first(m1.reterms))
```

In practice $\mathbf{Z}$ is stored and manipulated as a special type of sparse matrix.

The matrix $\Lambda$ is block-diagonal consisting of 18 diagonal blocks of size $2\times 2$, each of which is a copy of $\lambda$.  It could be written as a [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product)

```{julia}
Λ = kron(I(18), first(m1.λ))
```

but there is no need to actually construct $\bbLambda$.  It is completely determined by $\bblambda$.

## Spherical random effects

One of the many useful properties of the normal distribution is that a scalar normal distribution, $\mathcal{X}\sim\mathcal{N}(\mu,\sigma^2)$, can be expressed in terms of the _standard normal_ distribution, $\mathcal{Z}\sim\mathcal{N}(0,1)$ as
$$
\mathcal{X} = \mu + \sigma \mathcal{Z}
$$
A similar result holds for the multivariate normal distribution.  The random effects vector, $\mathcal{B}$, with distribution $\mathcal{N}(\mathbf{0},\bbSigma)$ can be generated from a "spherical" random effects vector, $\mathcal{U}$, as
$$
\mathcal{B} = \Lambda \mathcal{U}\quad\mathrm{where}\quad\mathcal{U}\sim\mathcal{N}(\mathbf{0},\sigma^2\mathbf{I}_q)
$$
and $q$ is the dimension of the random-effects vector (36 in our example).

(Recall that a multivariate normal distribution with covariance matrix $\sigma^2\mathbf{I}$ is called a "spherical normal" because the contours of constant probability density are spheres.  The random effects vector $\mathcal{U}$ has such a spherical distribution.)

Now the conditional distribution of the response, given the random effects, can be written in terms of $\mathcal{U}$ as
$$
(\mathcal{Y}|\mathcal{U}=\mathbf{u})\sim\mathcal{N}\left(\mathbf{X\bbbeta}+\mathbf{Z\Lambda u}, \sigma^2\mathbf{I}_n\right)
$$

The joint probability density for $\mathcal{Y}$ and $\mathcal{U}$ is the product of the conditional density of $\mathcal{Y}|\mathcal{U}=\mathbf{u}$ and the unconditional density of $\mathcal{U}$.
$$
\begin{aligned}
f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})&= \frac{1}{(2\pi\sigma^2)^{n/2}}\exp\left(-\frac{\|\mathbf{y}-\mathbf{X\bbbeta}-\mathbf{Z\Lambda u}\|^2}{2\sigma^2}\right)\,\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-\frac{\|\mathbf{u}\|^2}{2\sigma^2}\right)\\
&=\frac{1}{(2\pi\sigma^2)^{(n+q)/2}}\exp\left(-\frac{\|\mathbf{y}-\mathbf{X\bbbeta}-\mathbf{Z\Lambda u}\|^2+\|\mathbf{u}\|^2}{2\sigma^2}\right)
\end{aligned}
$$

Evaluating the likelihood requires the marginal distribution of $\mathcal{Y}$.  This can be obtained by integrating the joint distribution, $f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})$, evaluated at the observed $\mathbf{y}$, with respect to $\mathbf{u}$. There is an analytic solution to this integral.  To derive this solution, we first write the penalized sum of squared residuals in a somewhat unusual but very useful form.  Let $\bbtheta$ be the vector of parameters that determine $\bblambda$.

```{julia}
show(m1.θ)
```

In this case, $\bbtheta$ consists if the elements of the lower triangle of $\bblambda$.  What we will show is that, given a value of $\bbtheta$ the maximum of the log-likelihood for that value of $\bbtheta$ and any value of $\bbbeta$ and $\sigma$ can be determined from a matrix decomposition, specifically a Cholesky decomposition shown below.  This is called _profiling_ the log likelihood.

For the purposes of the optimization the objective is on the [_deviance_](https://en.wikipedia.org/wiki/Deviance_(statistics)) scale, which is negative twice the log-likelihood.  A summary of the optimization can be obtained as

```{julia}
m1.optsum
```

It required fewer than 60 evaluations of the objective function to obtain the maximum likelihood estimates of $\bbtheta$ and, with them, the estimates of all the other parameters.

To evaluate the log-likelihood we write the penalized sum of squared residuals in the joint density, $f_{\mathcal{Y},\mathcal{U}}(\mathbf{y,u})$, as
$$
\begin{aligned}
r^2_\bbtheta(\mathbf{u},\bbbeta) &=  \|\mathbf{y}-\mathbf{X\bbbeta}-\mathbf{Z\Lambda_\theta u}\|^2+\|\mathbf{u}\|^2\\
&=\left\|\begin{bmatrix}
\mathbf{Z\Lambda}&\mathbf{X}&\mathbf{y}\\
\mathbf{I}_q&\mathbf{0}&\mathbf{0}
\end{bmatrix}\begin{bmatrix}-\mathbf{u}\\ -\bbbeta \\ 1\end{bmatrix}\right\|^2 \\
&= \begin{bmatrix}-\mathbf{u}&-\bbbeta&1\end{bmatrix}
\begin{bmatrix}
\bbLambda^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \bbLambda^\prime\mathbf{Z}^\prime\mathbf{X} & \bbLambda^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\bbbeta \\ 1\end{bmatrix}\\
&=
\begin{bmatrix}-\mathbf{u}&-\bbbeta&1\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ}^\prime & \mathbf{0} & \mathbf{0} \\
\mathbf{R}_{ZX}^\prime & \mathbf{R}_{XX}^\prime & \mathbf{0} \\
\bbr_{Zy}^\prime & \bbr_{Xy}^\prime & r_{yy}
\end{bmatrix}
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \bbr_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \bbr_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\bbbeta \\ 1\end{bmatrix}\\
&= \left\|
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \bbr_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \bbr_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
\begin{bmatrix}-\mathbf{u}\\ -\bbbeta \\ 1\end{bmatrix}\right\|^2\\
&=\|\bbr_{Zy}-\mathbf{R}_{ZX}\bbbeta-\mathbf{R}_{ZZ}\mathbf{u}\|^2+ \|\bbr_{Xy}-\mathbf{R}_{XX}\bbbeta\|^2 + r_{yy}^2\\
&=r_{yy}^2+\|\mathbf{R}_{XX}\bbbeta-\bbr_{Xy}\|^2+\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\bbbeta-\bbr_{Zy}\|^2
\end{aligned}
$$
where
$$
\mathbf{R}(\bbtheta)=
\begin{bmatrix}
\mathbf{R}_{ZZ} & \mathbf{R}_{ZX} & \bbr_{Zy} \\
\mathbf{0} & \mathbf{R}_{XX} & \bbr_{Xy} \\
\mathbf{0} & \mathbf{0} & r_{yy}
\end{bmatrix}
$$
is the upper triangular, right Cholesky factor of the symmetric, positive definite matrix
$$
\begin{bmatrix}
\bbLambda^\prime\mathbf{Z}^\prime\mathbf{Z\Lambda}+\mathbf{I} & \bbLambda^\prime\mathbf{Z}^\prime\mathbf{X} & \bbLambda^\prime\mathbf{Z}^\prime\mathbf{y} \\
\mathbf{X}^\prime\mathbf{Z\Lambda} & \mathbf{X}^\prime\mathbf{X} & \mathbf{X}^\prime\mathbf{y} \\
\mathbf{y}^\prime\mathbf{Z\Lambda} & \mathbf{y}^\prime\mathbf{X} & \mathbf{y}^\prime\mathbf{y}
\end{bmatrix}
$$
The sub-matrices on the diagonal, $\mathbf{R}_{ZZ}$ and $\mathbf{R}_{XX}$, are upper triangular and $\mathbf{R}_{ZZ}$ is sparse.  In our example, $\mathbf{R}_{ZZ}$ is $36\times 36$ but the only non-zeros are the upper triangles of $18$ blocks of size $2\times 2$ along the diagonal.  Also, the diagonal elements are, by construction, positive.  Because $\mathbf{R}_{ZZ}$ is triangular its determinant, $|\mathbf{R}_{ZZ}|$, is the product of its diagonal elements which also must be positive.

Furthermore, we can see that, for a fixed value of $\bbtheta$ the minimum $r^2_\bbtheta(\mathbf{u},\bbbeta)$ is $r_{yy}^2$ and the conditional estimate of $\bbbeta$ satisfies
$$
\mathbf{R}_{XX}\widehat{\bbbeta}(\bbtheta)=\bbr_{Xy} .
$$
The conditional mode, $\tilde{\mathbf{u}}$, of $\mathcal{U}$ given $\mathcal{Y}=\mathbf{y}$ is the solution to
$$
\mathbf{R}_{ZZ}\tilde{\mathbf{u}}=\bbr_{Zy}-\mathbf{R}_{ZX}\bbbeta
$$
Technically, $\bbbeta$ and $\bbtheta$ are assumed known because this is a statement about distributions.  In practice, the estimates, $\widehat{\bbtheta}$ and $\widehat{\beta}$, are plugged in.

A Cholesky decomposition can be written in terms of the lower triangular factor on the left, $\bbL$, or in terms of $\mathbf{R}$ on the right.  There is a slight technical advantage in evaluating $\bbL$ in the `MixedModels` package so it is $\bbL$ that is evaluated and stored.  However, the theory is a bit easier to see in terms of $\mathbf{R}$, which we can obtain as

```{julia}
UpperTriangular(Matrix(sparseL(m1; full=true)'))
```

To evaluate the likelihood,
$$
L(\bbtheta,\bbbeta,\sigma|\mathbf{y}) = \int_\mathbf{u} f_{\mathcal{Y},\mathcal{U}}(\mathbf{y},\mathbf{u})\, d\mathbf{u}
$$
we isolate the part of the joint density that depends on $\mathbf{u}$ and perform a change of variable to
$$
\mathbf{v}=\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\bbbeta-\bbr_{Zy} .
$$
From the properties of the multivariate Gaussian distribution
$$
\begin{aligned}
\int_{\mathbf{u}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-
\frac{\|\mathbf{R}_{ZZ}\mathbf{u}+\mathbf{R}_{ZX}\bbbeta-\bbr_{Zy}\|^2}{2\sigma^2}\right)\,d\mathbf{u}
&=\int_{\mathbf{v}}\frac{1}{(2\pi\sigma^2)^{q/2}}\exp\left(-\frac{\|\mathbf{v}\|^2}{2\sigma^2}\right)|\mathbf{R}_{ZZ}|^{-1}\,d\mathbf{v}\\
&=|\mathbf{R}_{ZZ}|^{-1}
\end{aligned}
$$
from which we obtain the likelihood as
$$
L(\bbtheta,\bbbeta,\sigma)=\frac{|\mathbf{R}_{ZZ}|^{-1}}{(2\pi\sigma^2)^{n/2}}\exp\left(-
\frac{r_{yy}^2 + \|\mathbf{R}_{XX}(\bbbeta-\widehat{\bbbeta})\|^2}{2\sigma^2}\right)
$$
If we plug in $\bbbeta=\widehat{\bbbeta}$ and take the logarithm we can solve for the estimate of $\sigma^2$, given $\bbtheta$
$$
\widehat{\sigma^2}=\frac{r_{yy}^2}{n}
$$
which gives the _profiled log-likelihood_, $\ell(\bbtheta|\mathbf{y})=\log L(\bbtheta,\widehat{\bbbeta},\widehat{\sigma})$ as
$$
-2\ell(\bbtheta|\mathbf{y})=2\log(|\mathbf{R}_{ZZ}|) +
    n\left(1+\log\left(\frac{2\pi r_{yy}^2(\bbtheta)}{n}\right)\right)
$$

This may seem complicated but, relative to other formulations of the model, it is remarkably simple.

One of the interesting aspects of this formulation is that it is not necessary to solve for the conditional estimate of $\bbbeta$ or the conditional modes of the random effects when evaluating the log-likelihood.  The two values needed for the log-likelihood, $2\log(|\mathbf{R}_{ZZ}|)$ and $r_{yy}^2$ are obtained directly from the Cholesky factor.  The logarithm of the determinant,
$$
2\log(|\mathbf{R}_{ZZ}|) = \log(|\bbLambda^\prime\mathbf{Z}^\prime\mathbf{Z}\bbLambda+\mathbf{I}_q|)
$$
is available as

```{julia}
logdet(m1)
```

and $r_{yy}^2$ is available as

```{julia}
pwrss(m1)
```

which is the square of the element in the lower right corner of either $\bbL$ or $\mathbf{R}$

```{julia}
abs2(last(m1.L[3]))
```

Alternatively, `varest` returns $\widehat{\sigma^2}$

```{julia}
varest(m1)
```

This gives the objective function as

```{julia}
logdet(m1) + dof_residual(m1)*(1 + log(2π * varest(m1)))
```

One last technical point, the update of the Cholesky factor, $\bbL$, for a new value of $\bbtheta$, which generates $\bblambda$ and, hence, $\bbLambda$ can start with the model matrices $\mathbf{Z}$ and $\mathbf{X}$ and the response, $\mathbf{y}$ or it can start with the products, $\mathbf{Z}^\prime\mathbf{Z}$, etc.
The package uses the second approach which is more efficient when the number of observations is large relative to the number of random effects.  The non-redundant products are stored in the `A` field.

```julia
Symmetric(m1.A, :L)
```

Because the experiment is _balanced_, in the sense that each subject's reaction time is measured the same number of times and after the same number of days of sleep deprivation, the diagonal blocks in $\mathbf{Z}^\prime\mathbf{Z}$ are repetitions of one another.  The number in the lower right-hand corner of `A` is $\mathbf{y}^\prime\mathbf{y}$ or

```{julia}
sum(abs2, sleepstudy.reaction)
```

## Mixed-models and shrinkage of estimates

[John Tukey](https://en.wikipedia.org/wiki/John_Tukey) characterized the _regularization_ or _shrinkage_ aspects of mixed-effects models as _borrowing strength_ from the estimates for other subjects in the experiment.  The penalty term in the penalized least squares calculation has the effect of shrinking an individual's coefficients in the predictor back toward the global estimates.

```{julia}
#| code-fold: true
R"""
library(lme4)
library(lattice)
df <- coef(lme4::lmList(reaction ~ days | subj, $(DataFrame(sleepstudy))))
fm2 <- lme4::lmer(reaction ~ days + (days|subj), $(DataFrame(sleepstudy)))
fclow <- subset(df, `(Intercept)` < 251)
fchigh <- subset(df, `(Intercept)` > 251)
cc1 <- as.data.frame(coef(fm2)$subj)
names(cc1) <- c("A", "B")
df <- cbind(df, cc1)
ff <- lme4::fixef(fm2)
with(df,
     print(lattice::xyplot(`(Intercept)` ~ days, aspect = 1,
                  x1 = B, y1 = A,
                  panel = function(x, y, x1, y1, subscripts, ...) {
                      panel.grid(h = -1, v = -1)
                      x1 <- x1[subscripts]
                      y1 <- y1[subscripts]
                      larrows(x, y, x1, y1, type = "closed", length = 0.1,
                              angle = 15, ...)
                      lpoints(x, y,
                              pch = trellis.par.get("superpose.symbol")$pch[2],
                              col = trellis.par.get("superpose.symbol")$col[2])
                      lpoints(x1, y1,
                              pch = trellis.par.get("superpose.symbol")$pch[1],
                              col = trellis.par.get("superpose.symbol")$col[1])
                      lpoints(ff[2], ff[1],
                              pch = trellis.par.get("superpose.symbol")$pch[3],
                              col = trellis.par.get("superpose.symbol")$col[3])
                      ltext(fclow[,2], fclow[,1], row.names(fclow),
                            adj = c(0.5, 1.7))
                      ltext(fchigh[,2], fchigh[,1], row.names(fchigh),
                            adj = c(0.5, -0.6))
                  },
                  key = list(space = "top", columns = 3,
                  text = list(c("Mixed model", "Within-group", "Population")),
                  points = list(col = trellis.par.get("superpose.symbol")$col[1:3],
                  pch = trellis.par.get("superpose.symbol")$pch[1:3]))
                  )))
""";
```

```{julia}
#| code-fold: true
#| fig-cap: Shrinkage plot for model m1.
shrinkageplot(m1)
```

Comparing this plot to the original data plot with the lines from the various fits superimposed

```{julia}
#| code-fold: true
R"""
print(xyplot(Reaction ~ Days | Subject, sleepstudy, aspect = "xy",
             layout = c(9,2), type = c("g", "p", "r"),
             coef.list = df[,3:4],
             panel = function(..., coef.list) {
                 panel.xyplot(...)
                 panel.abline(as.numeric(coef.list[packet.number(),]),
                              col.line = trellis.par.get("superpose.line")$col[2],
                              lty = trellis.par.get("superpose.line")$lty[2]
                              )
                 panel.abline(fixef(fm2),
                              col.line = trellis.par.get("superpose.line")$col[4],
                              lty = trellis.par.get("superpose.line")$lty[4]
                              )
             },
             index.cond = function(x,y) coef(lm(y ~ x))[1],
             xlab = "Days of sleep deprivation",
             ylab = "Average reaction time (ms)",
             key = list(space = "top", columns = 3,
             text = list(c("Within-subject", "Mixed model", "Population")),
             lines = list(col = trellis.par.get("superpose.line")$col[c(2:1,4)],
             lty = trellis.par.get("superpose.line")$lty[c(2:1,4)]))))
""";
```

shows that the fits for those subjects whose data shows a strong linear trend (e.g. 308, 309, 310, 337) are not changed that much.  But those whose data does not define a line well (e.g. 330, 331) are shrunk toward the global fit.
